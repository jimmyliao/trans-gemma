# TranslateGemma ç¯„ä¾‹èˆ‡æ¸¬è©¦å·¥å…·

é€™å€‹ç›®éŒ„åŒ…å« TranslateGemma çš„ä½¿ç”¨ç¯„ä¾‹å’Œæ¸¬è©¦å·¥å…·ã€‚

## ğŸ“ ç›®éŒ„çµæ§‹

```
examples/
â”œâ”€â”€ translate.py              # â­ çµ±ä¸€ç¿»è­¯å·¥å…·ï¼ˆå…¨æ–°ï¼ï¼‰
â”œâ”€â”€ backends/                 # ç¿»è­¯å¾Œç«¯å¯¦ä½œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # æŠ½è±¡åŸºç¤é¡åˆ¥
â”‚   â”œâ”€â”€ transformers_backend.py  # Hugging Face Transformers
â”‚   â”œâ”€â”€ ollama_backend.py     # Ollama (Metal å„ªåŒ–)
â”‚   â””â”€â”€ mlx_backend.py        # MLX (Apple Silicon å„ªåŒ–)
â”œâ”€â”€ verify-hf-token.py        # é©—è­‰ Hugging Face token
â”œâ”€â”€ simple-translation.py     # Cloud Run API å®¢æˆ¶ç«¯
â”œâ”€â”€ local-test.py             # èˆŠç‰ˆï¼šå®Œæ•´ transformers æ¸¬è©¦
â””â”€â”€ translategemma-fix.py     # èˆŠç‰ˆï¼šTranslateGemma ä¿®æ­£æ¸¬è©¦
```

## ğŸš€ çµ±ä¸€ç¿»è­¯å·¥å…· (translate.py)

**æ¨è–¦**çš„æœ¬åœ°ä½¿ç”¨ TranslateGemma æ–¹å¼ã€‚

### ç‰¹è‰²

- **å¤šç¨®å¾Œç«¯**ï¼šå¯é¸æ“‡ transformersã€ollama æˆ– mlx
- **å…©ç¨®æ¨¡å¼**ï¼šå–®æ¬¡ç¿»è­¯æˆ–äº’å‹•å¼ REPL
- **æ˜“æ–¼ä½¿ç”¨**ï¼šç°¡å–®çš„å‘½ä»¤åˆ—ä»‹é¢
- **å„ªåŒ–**ï¼šæ¯å€‹å¾Œç«¯éƒ½é‡å°å…¶ç›®æ¨™å¹³å°å„ªåŒ–

### å¿«é€Ÿé–‹å§‹

```bash
# å–®æ¬¡ç¿»è­¯ï¼ˆOllama - M1 ä¸Šæœ€å¿«ï¼‰
./run-examples.sh translate --text "Hello, world!" --backend ollama

# äº’å‹•æ¨¡å¼
./run-examples.sh translate --mode interactive --backend ollama

# ä½¿ç”¨ transformers å¾Œç«¯
./run-examples.sh translate --text "Hello!" --backend transformers --target ja

# ä½¿ç”¨ MLX å¾Œç«¯ï¼ˆApple Silicon å„ªåŒ–ï¼‰
./run-examples.sh translate --text "Hello!" --backend mlx
```

### å¾Œç«¯æ¯”è¼ƒ

| å¾Œç«¯ | é€Ÿåº¦ | å®‰è£ | ç‹€æ…‹ | æœ€é©åˆ |
|------|------|------|------|--------|
| **ollama** | âš¡âš¡âš¡ å¿« | ä¸€è¡ŒæŒ‡ä»¤ | âœ… æ¨è–¦ | M1/M2/M3 Macï¼Œæ‰€æœ‰ç”¨é€” |
| **transformers** | âš ï¸ æ…¢ | é è¨­ | âœ… å¯ç”¨ | Colab/CUDA GPUï¼Œç ”ç©¶ç”¨é€” |
| **mlx** | - | - | âŒ ä¸å¯ç”¨ | ç­‰å¾… MLX ç‰ˆæœ¬æ¨¡å‹ |

**æ³¨æ„**: MLX å¾Œç«¯æš«æ™‚ä¸å¯ç”¨ï¼Œå› ç‚º TranslateGemma å°šæœªæœ‰ MLX å„ªåŒ–ç‰ˆæœ¬ã€‚

### æ¨¡å¼

#### 1. å–®æ¬¡æ¨¡å¼ï¼ˆOne-shotï¼Œé è¨­ï¼‰

ç¿»è­¯å–®ä¸€æ–‡å­—å¾Œé€€å‡ºã€‚

```bash
# åŸºæœ¬ä½¿ç”¨
./run-examples.sh translate --text "æ—©å®‰ï¼"

# æŒ‡å®šç›®æ¨™èªè¨€
./run-examples.sh translate --text "Hello!" --target ja

# æŒ‡å®šä¾†æºèªè¨€
./run-examples.sh translate --text "Bonjour" --source fr --target en
```

#### 2. äº’å‹•æ¨¡å¼ï¼ˆInteractiveï¼‰

REPL æ¨¡å¼æŒçºŒç¿»è­¯ã€‚

```bash
./run-examples.sh translate --mode interactive
```

**äº’å‹•å‘½ä»¤ï¼š**

- `:target <code>` - æ›´æ”¹ç›®æ¨™èªè¨€
- `:source <code>` - æ›´æ”¹ä¾†æºèªè¨€
- `:info` - é¡¯ç¤ºå¾Œç«¯è³‡è¨Š
- `:quit`, `:exit` - é€€å‡º

**ç¯„ä¾‹æœƒè©±ï¼š**

```
[en â†’ zh-TW] Hello, world!
â†’ ä½ å¥½ï¼Œä¸–ç•Œï¼
  (0.82s, 3.7 tok/s)

[en â†’ zh-TW] :target ja
â„¹ï¸  Target language changed to: ja

[en â†’ ja] Good morning!
â†’ ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼
  (1.2s, 5.1 tok/s)

[en â†’ ja] :quit

Statistics:
  Total translations: 2
  Average time: 1.01s

âœ… Goodbye!
```

### ç’°å¢ƒè®Šæ•¸

- `BACKEND`: é è¨­å¾Œç«¯ (`transformers`, `ollama`, `mlx`)
- `FORCE_DEVICE`: transformers çš„è¨­å‚™ (`cpu`, `mps`, `auto`)
- `NO_MEM_LIMIT`: åœç”¨ transformers è¨˜æ†¶é«”é™åˆ¶ (`0`, `1`)

### ç¯„ä¾‹

```bash
# Ollamaï¼ˆæ¨è–¦çµ¦ M1/M2/M3 Macï¼‰
./run-examples.sh translate --text "Hello!" --backend ollama

# Transformers ä½¿ç”¨ CPU
FORCE_DEVICE=cpu ./run-examples.sh translate --text "Hello!" --backend transformers

# äº’å‹•æ¨¡å¼
./run-examples.sh translate --mode interactive --backend ollama
```

## ğŸ†š æ•ˆèƒ½æ¯”è¼ƒï¼ˆM1 Macï¼‰

åŸºæ–¼å¯¦éš›æ¸¬è©¦ï¼š

| å¾Œç«¯ | æ¨¡å‹è¼‰å…¥ | é¦–æ¬¡ç¿»è­¯ | è¨˜æ†¶é«” | ç‹€æ…‹ |
|------|----------|----------|--------|------|
| Ollama | 0.04s | 0.8s | 3.3 GB | âœ… æ¨è–¦ |
| Transformers (MPS 8GB) | 8.8s | 94.8s âš ï¸ | 8.7 GB | âš ï¸ å¤ªæ…¢ |
| Transformers (CPU) | ~15s | ~5min âš ï¸ | 10 GB | âš ï¸ éå¸¸æ…¢ |

**çµè«–**ï¼š**åœ¨ M1 Mac ä¸Šä½¿ç”¨ Ollama**ã€‚Transformers å¤ªæ…¢ä¸å¯¦ç”¨ã€‚

## ğŸ“ å…¶ä»–ç¯„ä¾‹

### verify-hf-token.py

é©—è­‰ä½ çš„ Hugging Face token å’Œæ¨¡å‹å­˜å–æ¬Šé™ã€‚

```bash
./run-examples.sh verify-hf-token
```

### simple-translation.py

Cloud Run API å®¢æˆ¶ç«¯ç¯„ä¾‹ï¼ˆéœ€è¦å·²éƒ¨ç½²çš„æœå‹™ï¼‰ã€‚

```bash
./run-examples.sh simple-translation
```

### èˆŠç‰ˆè…³æœ¬

**local-test.py** å’Œ **translategemma-fix.py** æ˜¯æ¸¬è©¦ transformers å¾Œç«¯çš„èˆŠç‰ˆè…³æœ¬ã€‚æ–°é–‹ç™¼è«‹ä½¿ç”¨ `translate.py`ã€‚

## ğŸ—ï¸ å¾Œç«¯æ¶æ§‹

æ‰€æœ‰å¾Œç«¯éƒ½å¯¦ä½œ `TranslationBackend` ä»‹é¢ï¼š

```python
class TranslationBackend(ABC):
    def load_model(self, **kwargs) -> Dict[str, Any]:
        """è¼‰å…¥æ¨¡å‹ä¸¦å›å‚³å…ƒè³‡æ–™"""
        pass

    def translate(self, text: str, source_lang: str, target_lang: str) -> Dict[str, Any]:
        """ç¿»è­¯æ–‡å­—"""
        pass

    def get_backend_info(self) -> Dict[str, str]:
        """å–å¾—å¾Œç«¯è³‡è¨Š"""
        pass

    def cleanup(self):
        """é¸æ“‡æ€§çš„æ¸…ç†æ–¹æ³•"""
        pass
```

### æ–°å¢è‡ªè¨‚å¾Œç«¯

1. å»ºç«‹ `backends/your_backend.py`
2. å¯¦ä½œ `TranslationBackend` ä»‹é¢
3. åœ¨ `backends/__init__.py` ä¸­è¨»å†Š
4. ä½¿ç”¨ `translate.py --backend your_backend` æ¸¬è©¦

## ğŸš¨ é‡è¦ï¼šTranslateGemma Chat Template æ ¼å¼

TranslateGemma ä½¿ç”¨**ç‰¹æ®Šçš„ chat template æ ¼å¼**ï¼Œèˆ‡æ¨™æº–æ ¼å¼ä¸åŒã€‚

### âŒ éŒ¯èª¤æ ¼å¼ï¼ˆæœƒå°è‡´ TemplateErrorï¼‰ï¼š
```python
messages = [
    {
        "role": "user",
        "content": "Translate this to Chinese: Hello"
    }
]
```

### âœ… æ­£ç¢ºæ ¼å¼ï¼š
```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Hello",
                "source_lang_code": "en",
                "target_lang_code": "zh-TW"
            }
        ]
    }
]
```

## ğŸ¯ èªè¨€ä»£ç¢¼å°ç…§è¡¨

ä½¿ç”¨ **ISO 639-1ï¼ˆå…©ç¢¼ï¼‰** æ¨™æº–ï¼š

- `en` - English
- `zh-TW` - Traditional Chinese (Taiwan) ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰
- `zh-CN` - Simplified Chinese ç°¡é«”ä¸­æ–‡
- `ja` - Japanese æ—¥æ–‡
- `ko` - Korean éŸ“æ–‡
- `es` - Spanish è¥¿ç­ç‰™æ–‡
- `fr` - French æ³•æ–‡
- `de` - German å¾·æ–‡
- ç­‰ç­‰

**é‡è¦**ï¼šä½¿ç”¨å…©ç¢¼æ ¼å¼ï¼ˆå¦‚ `en`ï¼‰ï¼Œä¸æ˜¯ä¸‰ç¢¼æ ¼å¼ï¼ˆå¦‚ `eng`ï¼‰

## ğŸ› ç–‘é›£æ’è§£

### Ollama: Model not found

```bash
ollama pull translategemma
```

### MLX: Backend not available

MLX å¾Œç«¯ç›®å‰ä¸å¯ç”¨ï¼Œå› ç‚º TranslateGemma å°šæœªæœ‰ MLX å„ªåŒ–ç‰ˆæœ¬ã€‚è«‹ä½¿ç”¨ Ollama å¾Œç«¯ï¼š

```bash
./run-examples.sh translate --text "Hello!" --backend ollama
```

### Transformers: Invalid buffer size (M1 Mac)

åœ¨ M1 Mac ä¸Šè«‹æ”¹ç”¨ Ollamaã€‚Transformers åœ¨ MPS ä¸Šæœ‰è¨˜æ†¶é«”ç®¡ç†å•é¡Œï¼Œæ•ˆèƒ½ä¹Ÿè¼ƒå·®ã€‚

## ğŸ“ .env æª”æ¡ˆè¨­å®š

è¤‡è£½ `.env.example` åˆ° `.env` ä¸¦å¡«å…¥ä½ çš„é…ç½®ï¼š

```bash
# Hugging Face Access Token
HF_TOKEN=hf_xxxxx

# GCP Project (for Cloud Run deployment)
PROJECT_ID=your-gcp-project-id
REGION=us-central1
SERVICE_NAME=translategemma-4b

# Model Configuration
MODEL_ID=google/translategemma-4b-it
```

**é‡è¦**ï¼š
- âš ï¸ `.env` æª”æ¡ˆåŒ…å«æ•æ„Ÿè³‡è¨Šï¼Œ**ä¸è¦** commit åˆ° Git
- âœ… `.env` å·²ç¶“åœ¨ `.gitignore` ä¸­
- âœ… ä½¿ç”¨ `.env.example` ä½œç‚ºç¯„æœ¬

## ğŸ”— ç›¸é—œè³‡æº

- [TranslateGemma å®˜æ–¹é é¢](https://huggingface.co/google/translategemma-4b-it)
- [Ollama å®˜æ–¹ç¶²ç«™](https://ollama.ai/)
- [MLX GitHub](https://github.com/ml-explore/mlx)
- [å°ˆæ¡ˆ GitHub Repository](https://github.com/jimmyliao/trans-gemma)

## ğŸ†˜ å¸¸è¦‹å•é¡Œ

### Q: æ‡‰è©²ä½¿ç”¨å“ªå€‹å¾Œç«¯ï¼Ÿ

A:
- **M1/M2/M3 Mac**: ä½¿ç”¨ Ollamaï¼ˆæ¨è–¦ï¼‰
- **Google Colab / NVIDIA GPU**: ä½¿ç”¨ Transformers
- **CPU only**: ä½¿ç”¨ Ollamaï¼ˆå¦‚å·²å®‰è£ï¼‰æˆ– Transformers CPU æ¨¡å¼

### Q: Ollama æœƒä½¿ç”¨ GPU å—ï¼Ÿ

A: æ˜¯çš„ï¼ŒOllama åœ¨ M1 ä¸Šè‡ªå‹•ä½¿ç”¨ Metal (GPU) åŠ é€Ÿã€‚

### Q: ç‚ºä»€éº¼ Transformers é€™éº¼æ…¢ï¼Ÿ

A: Transformers åœ¨ M1 çš„ MPS ä¸Šæ”¯æ´ä¸ä½³ï¼Œæœ‰è¨˜æ†¶é«”ç®¡ç†å•é¡Œã€‚æ¨è–¦ä½¿ç”¨é‡å° M1 å„ªåŒ–çš„ Ollama æˆ– MLXã€‚

### Q: å¦‚ä½•å–å¾— HF_TOKENï¼Ÿ

A: å‰å¾€ https://huggingface.co/settings/tokens å»ºç«‹æ–° tokenï¼Œé¸æ“‡ Read æ¬Šé™å³å¯ã€‚
