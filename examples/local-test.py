"""
æœ¬åœ°æ¸¬è©¦è…³æœ¬ - ä½¿ç”¨ .env æª”æ¡ˆå„²å­˜ HF_TOKEN

ä½¿ç”¨æ–¹å¼ï¼š
1. è¤‡è£½ .env.example åˆ° .env
2. åœ¨ .env ä¸­å¡«å…¥ä½ çš„ HF_TOKEN
3. åŸ·è¡Œ: python examples/local-test.py
"""

import os
import sys
import time
import psutil
import shutil
import warnings
from datetime import datetime
from pathlib import Path

# æŠ‘åˆ¶ transformers çš„è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# é¡è‰²ä»£ç¢¼
class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    BOLD = '\033[1m'
    NC = '\033[0m'  # No Color

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é€²åº¦é¡¯ç¤ºå·¥å…·å‡½æ•¸
def print_step(step, message, **kwargs):
    """é¡¯ç¤ºæ­¥é©Ÿé€²åº¦"""
    timestamp = kwargs.get('timestamp', False)
    memory = kwargs.get('memory', False)
    disk = kwargs.get('disk', False)

    prefix = f"{Colors.BLUE}[{step}]{Colors.NC}"

    # æ™‚é–“æˆ³è¨˜
    time_str = ""
    if timestamp:
        time_str = f" {Colors.CYAN}({datetime.now().strftime('%H:%M:%S')}){Colors.NC}"

    print(f"{prefix} {message}{time_str}")

    # è¨˜æ†¶é«”ä½¿ç”¨
    if memory:
        mem = psutil.virtual_memory()
        mem_used_gb = mem.used / (1024**3)
        mem_total_gb = mem.total / (1024**3)
        mem_percent = mem.percent
        print(f"   ğŸ’¾ è¨˜æ†¶é«”: {mem_used_gb:.1f}GB / {mem_total_gb:.1f}GB ({mem_percent}%)")

    # ç£ç¢Ÿç©ºé–“
    if disk:
        disk_usage = shutil.disk_usage("/")
        disk_free_gb = disk_usage.free / (1024**3)
        disk_total_gb = disk_usage.total / (1024**3)
        disk_percent = (disk_usage.used / disk_usage.total) * 100
        print(f"   ğŸ’¿ ç£ç¢Ÿç©ºé–“: {disk_free_gb:.1f}GB å¯ç”¨ / {disk_total_gb:.1f}GB ç¸½è¨ˆ ({100-disk_percent:.1f}% å¯ç”¨)")

def print_success(message, detail=None):
    """é¡¯ç¤ºæˆåŠŸè¨Šæ¯"""
    print(f"{Colors.GREEN}âœ… {message}{Colors.NC}")
    if detail:
        print(f"   {Colors.CYAN}{detail}{Colors.NC}")

def print_error(message, detail=None):
    """é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯"""
    print(f"{Colors.RED}âŒ {message}{Colors.NC}")
    if detail:
        print(f"   {detail}")

def print_warning(message):
    """é¡¯ç¤ºè­¦å‘Šè¨Šæ¯"""
    print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.NC}")

def check_model_cache(model_id):
    """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¶“ä¸‹è¼‰åˆ°å¿«å–"""
    from huggingface_hub import scan_cache_dir

    try:
        cache_info = scan_cache_dir()

        # å°‹æ‰¾æ¨¡å‹
        for repo in cache_info.repos:
            if model_id in repo.repo_id:
                # è¨ˆç®—æ¨¡å‹å¤§å°
                total_size = sum(revision.size_on_disk for revision in repo.revisions)
                size_gb = total_size / (1024**3)

                # æª¢æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¸‹è¼‰
                incomplete_files = []
                cache_path = Path.home() / ".cache" / "huggingface" / "hub"
                model_cache_dir = cache_path / f"models--{model_id.replace('/', '--')}"

                if model_cache_dir.exists():
                    incomplete_files = list(model_cache_dir.rglob("*.incomplete"))

                if incomplete_files:
                    return {
                        "cached": False,
                        "partial": True,
                        "size_gb": size_gb,
                        "incomplete_count": len(incomplete_files)
                    }
                else:
                    return {
                        "cached": True,
                        "partial": False,
                        "size_gb": size_gb
                    }

        # æ¨¡å‹æœªæ‰¾åˆ°
        return {"cached": False, "partial": False, "size_gb": 0}

    except Exception as e:
        # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œå‡è¨­æœªä¸‹è¼‰
        return {"cached": False, "partial": False, "size_gb": 0, "error": str(e)}

def load_env():
    """è¼‰å…¥ .env æª”æ¡ˆ"""
    env_file = project_root / ".env"

    if not env_file.exists():
        print("âŒ .env æª”æ¡ˆä¸å­˜åœ¨")
        print()
        print("è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š")
        print("1. è¤‡è£½ .env.example åˆ° .env:")
        print("   cp .env.example .env")
        print()
        print("2. ç·¨è¼¯ .env ä¸¦å¡«å…¥ä½ çš„ HF_TOKEN:")
        print("   # å¾ https://huggingface.co/settings/tokens å–å¾—")
        print("   HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
        return False

    # è®€å– .env æª”æ¡ˆ
    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

    return True

def test_token():
    """æ¸¬è©¦ HF_TOKEN æ˜¯å¦æœ‰æ•ˆ"""
    token = os.getenv("HF_TOKEN")

    if not token or token.startswith("hf_xxx"):
        print("âŒ HF_TOKEN æœªè¨­å®šæˆ–ä½¿ç”¨é è¨­å€¼")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæœ‰æ•ˆçš„ HF_TOKEN")
        return False

    print("âœ… HF_TOKEN å·²è¨­å®š")
    print(f"   Token: {token[:10]}...{token[-5:]}")

    try:
        from huggingface_hub import login
        login(token=token)
        print("âœ… Hugging Face èªè­‰æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ Hugging Face èªè­‰å¤±æ•—: {e}")
        return False

def test_model_access():
    """æ¸¬è©¦æ¨¡å‹å­˜å–"""
    print("\næ¸¬è©¦ TranslateGemma æ¨¡å‹å­˜å–...")

    try:
        from transformers import AutoTokenizer

        MODEL_ID = os.getenv("MODEL_ID", "google/translategemma-4b-it")
        print(f"è¼‰å…¥ tokenizer: {MODEL_ID}")

        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        print(f"âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
        print(f"   è©å½™è¡¨å¤§å°: {len(tokenizer)}")
        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹å­˜å–å¤±æ•—: {e}")
        print()
        print("å¯èƒ½çš„åŸå› ï¼š")
        print("1. å°šæœªç”³è«‹ TranslateGemma å­˜å–æ¬Šé™")
        print("   å‰å¾€: https://huggingface.co/google/translategemma-4b-it")
        print("2. Token æ¬Šé™ä¸è¶³ï¼ˆéœ€è¦ Read æ¬Šé™ï¼‰")
        return False

def test_translation():
    """æ¸¬è©¦ç¿»è­¯åŠŸèƒ½"""
    print("\n" + "="*80)
    print_step("4/4", "é–‹å§‹ç¿»è­¯åŠŸèƒ½æ¸¬è©¦", timestamp=True, memory=True, disk=True)
    print("="*80)

    start_time = time.time()

    try:
        # è¨­å®š transformers æ—¥èªŒç­‰ç´šï¼Œæ¸›å°‘ä¸å¿…è¦çš„è­¦å‘Š
        os.environ["TRANSFORMERS_VERBOSITY"] = "error"

        # é¡å¤–è¨­å®š logging level
        import logging
        logging.getLogger("transformers").setLevel(logging.ERROR)

        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        MODEL_ID = os.getenv("MODEL_ID", "google/translategemma-4b-it")

        # æ­¥é©Ÿ 1: è¼‰å…¥ Tokenizer
        print()
        print_step("4.1", f"è¼‰å…¥ Tokenizer: {MODEL_ID}", timestamp=True)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        print_success("Tokenizer è¼‰å…¥æˆåŠŸ", f"è©å½™è¡¨å¤§å°: {len(tokenizer):,}")

        # æ­¥é©Ÿ 2: æª¢æŸ¥æ¨¡å‹å¿«å–ä¸¦è¼‰å…¥
        print()
        print_step("4.2", "æª¢æŸ¥æ¨¡å‹å¿«å–ç‹€æ…‹", timestamp=True)

        cache_status = check_model_cache(MODEL_ID)

        if cache_status.get("cached"):
            # æ¨¡å‹å·²å®Œæ•´ä¸‹è¼‰
            print_success("æ¨¡å‹å·²åœ¨å¿«å–ä¸­",
                         f"å¤§å°: {cache_status['size_gb']:.1f} GB, ç„¡éœ€é‡æ–°ä¸‹è¼‰")
        elif cache_status.get("partial"):
            # æœ‰æœªå®Œæˆçš„ä¸‹è¼‰
            print_warning(f"ç™¼ç¾ {cache_status['incomplete_count']} å€‹æœªå®Œæˆçš„ä¸‹è¼‰")
            print(f"   ç›®å‰å·²ä¸‹è¼‰: {cache_status['size_gb']:.1f} GB")
            print(f"   å»ºè­°å…ˆåŸ·è¡Œæ¸…ç†: ./run-examples.sh cleanup")
        else:
            # æ¨¡å‹æœªä¸‹è¼‰
            print_warning("æ¨¡å‹æœªä¸‹è¼‰ï¼Œå°‡å¾ Hugging Face ä¸‹è¼‰ï¼ˆç´„ 8.6 GBï¼‰")

            # æª¢æŸ¥ç£ç¢Ÿç©ºé–“æ˜¯å¦è¶³å¤ 
            disk = shutil.disk_usage("/")
            free_gb = disk.free / (1024**3)

            if free_gb < 10:
                print_error(f"ç£ç¢Ÿç©ºé–“ä¸è¶³ï¼åƒ…å‰© {free_gb:.1f} GB",
                           "å»ºè­°è‡³å°‘æœ‰ 12 GB å¯ç”¨ç©ºé–“")
                print(f"   è«‹åŸ·è¡Œ: ./run-examples.sh cleanup")
                return False

        print()
        print_step("4.2b", "è¼‰å…¥æ¨¡å‹åˆ°è¨˜æ†¶é«”ï¼ˆå¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰",
                   timestamp=True, memory=True, disk=True)

        # æª¢æŸ¥ç©ºé–“ï¼šå³ä½¿æ¨¡å‹å·²ä¸‹è¼‰ï¼Œè¼‰å…¥æ™‚ä»éœ€è¦è‡¨æ™‚ç©ºé–“
        disk = shutil.disk_usage("/")
        free_gb = disk.free / (1024**3)

        # è¨ˆç®—æ‰€éœ€ç©ºé–“
        if cache_status.get("cached"):
            # æ¨¡å‹å·²ä¸‹è¼‰ï¼Œåªéœ€è¦è¼ƒå°‘çš„è‡¨æ™‚ç©ºé–“ï¼ˆ2-3 GBï¼‰
            required_gb = 3.0
            space_purpose = "è¼‰å…¥è‡¨æ™‚ç©ºé–“"
        else:
            # éœ€è¦ä¸‹è¼‰æ¨¡å‹ï¼ˆ8.6 GBï¼‰+ è‡¨æ™‚ç©ºé–“ï¼ˆ2-3 GBï¼‰
            required_gb = 12.0
            space_purpose = "ä¸‹è¼‰ + è¼‰å…¥"

        if free_gb < required_gb:
            print_error(
                f"ç£ç¢Ÿç©ºé–“ä¸è¶³ï¼åƒ…å‰© {free_gb:.1f} GB",
                f"å»ºè­°è‡³å°‘æœ‰ {required_gb:.1f} GB å¯ç”¨ç©ºé–“ï¼ˆ{space_purpose}ï¼‰"
            )
            print()
            print(f"{Colors.CYAN}è§£æ±ºæ–¹æ¡ˆï¼š{Colors.NC}")
            print(f"   1. åŸ·è¡Œæ¸…ç†: ./run-examples.sh cleanup")
            print(f"   2. æ¸…ç†ç³»çµ±æš«å­˜: sudo rm -rf /private/var/tmp/*")
            print(f"   3. æ”¹ç”¨ Colabï¼ˆæ¨è–¦ï¼‰")
            return False

        if not cache_status.get("cached"):
            print_warning("é¦–æ¬¡ä¸‹è¼‰æ™‚ Hugging Face æœƒé¡¯ç¤ºé€²åº¦æ¢")
            print()

        model_start_time = time.time()

        # æª¢æŸ¥å¯ç”¨è¨˜æ†¶é«”ï¼Œæ±ºå®šè¼‰å…¥ç­–ç•¥
        mem = psutil.virtual_memory()
        available_mem_gb = mem.available / (1024**3)

        if available_mem_gb < 10:
            # è¨˜æ†¶é«”ä¸è¶³ï¼Œä½¿ç”¨ CPU-only æ¨¡å¼ï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰
            print_warning(f"å¯ç”¨è¨˜æ†¶é«”ä¸è¶³ ({available_mem_gb:.1f}GB < 10GB)")
            print(f"   {Colors.CYAN}ä½¿ç”¨ CPU-only æ¨¡å¼ï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰{Colors.NC}")
            device_map = "cpu"
            torch_dtype = torch.float32  # CPU ä¸æ”¯æ´ bfloat16
        else:
            # è¨˜æ†¶é«”å……è¶³ï¼Œä½¿ç”¨ auto (MPS æˆ– CUDA)
            device_map = "auto"
            torch_dtype = torch.bfloat16

        print(f"   è¼‰å…¥é…ç½®: device_map={device_map}, dtype={torch_dtype}")
        print()

        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            torch_dtype=torch_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True  # æ¸›å°‘è‡¨æ™‚æª”æ¡ˆå’Œè¨˜æ†¶é«”ä½¿ç”¨
        )
        model_load_time = time.time() - model_start_time

        print()
        device_info = f"device: {model.device}, dtype: {model.dtype}"
        print_success("æ¨¡å‹è¼‰å…¥æˆåŠŸ",
                     f"{device_info}, è€—æ™‚: {model_load_time:.1f} ç§’")

        # é¡¯ç¤ºæ¨¡å‹è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ç‹€æ…‹
        mem = psutil.virtual_memory()
        print(f"   ğŸ’¾ ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨: {mem.used / (1024**3):.1f}GB ({mem.percent}%)")

        # æ­¥é©Ÿ 3: æº–å‚™ç¿»è­¯
        print()
        print_step("4.3", "æº–å‚™ç¿»è­¯æ¸¬è©¦ï¼ˆè‹±æ–‡â†’ç¹é«”ä¸­æ–‡ï¼‰", timestamp=True)
        text = "Hello, world!"
        print(f"   åŸæ–‡: {text}")

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": text,
                        "source_lang_code": "en",
                        "target_lang_code": "zh-TW"
                    }
                ]
            }
        ]

        inputs = tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            add_generation_prompt=True
        ).to(model.device)

        print(f"   è¼¸å…¥ tokens: {inputs.shape[1]}")

        # æ­¥é©Ÿ 4: åŸ·è¡Œç¿»è­¯
        print()
        print_step("4.4", "åŸ·è¡Œç¿»è­¯æ¨ç†", timestamp=True)

        gen_start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=128,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
                # æ˜ç¢ºè¨­å®šç‚º None ä»¥é¿å…è­¦å‘Š
                top_p=None,
                top_k=None
            )
        gen_time = time.time() - gen_start_time

        # åªè§£ç¢¼æ–°ç”Ÿæˆçš„ tokensï¼ˆä¸åŒ…æ‹¬è¼¸å…¥ promptï¼‰
        # é€™æ¨£å°±ä¸æœƒé¡¯ç¤º system prompt
        generated_tokens = outputs[0][inputs.shape[1]:]
        translation = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        # å¦‚æœéœ€è¦çœ‹å®Œæ•´è¼¸å‡ºï¼ˆé™¤éŒ¯ç”¨ï¼‰
        if os.getenv("DEBUG_TRANSLATION"):
            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"\n{Colors.YELLOW}[DEBUG] å®Œæ•´è¼¸å‡º:{Colors.NC}")
            print(full_output)

        print()
        print(f"{Colors.BOLD}ç¿»è­¯çµæœ:{Colors.NC}")
        print(f"   åŸæ–‡: {text}")
        print(f"   è­¯æ–‡: {translation}")
        print(f"   æ¨ç†æ™‚é–“: {gen_time:.2f} ç§’")
        print(f"   ç”Ÿæˆ tokens: {len(generated_tokens)} (ç¸½è¨ˆ: {outputs.shape[1]})")
        print(f"   ç”Ÿæˆé€Ÿåº¦: {len(generated_tokens) / gen_time:.1f} tokens/ç§’")

        # ç¸½çµ
        total_time = time.time() - start_time
        print()
        print("="*80)
        print_success("ç¿»è­¯æ¸¬è©¦å®Œæˆ", f"ç¸½è€—æ™‚: {total_time:.1f} ç§’")
        print("="*80)

        return True

    except Exception as e:
        print()
        print_error("ç¿»è­¯æ¸¬è©¦å¤±æ•—", str(e))
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print()
    print("="*80)
    print(f"{Colors.BOLD}TranslateGemma æœ¬åœ°æ¸¬è©¦{Colors.NC}")
    print("="*80)
    print()

    # é¡¯ç¤ºç³»çµ±è³‡è¨Š
    mem = psutil.virtual_memory()
    disk = shutil.disk_usage("/")
    print(f"ğŸ’» ç³»çµ±è³‡è¨Š:")
    print(f"   è¨˜æ†¶é«”: {mem.total / (1024**3):.1f}GB ç¸½è¨ˆ, {mem.available / (1024**3):.1f}GB å¯ç”¨")
    print(f"   ç£ç¢Ÿ: {disk.free / (1024**3):.1f}GB å¯ç”¨ / {disk.total / (1024**3):.1f}GB ç¸½è¨ˆ")
    print()

    # 1. è¼‰å…¥ .env
    print_step("1/4", "è¼‰å…¥ .env æª”æ¡ˆ", timestamp=True)
    if not load_env():
        return 1
    print_success(".env æª”æ¡ˆè¼‰å…¥æˆåŠŸ")
    print()

    # 2. æ¸¬è©¦ token
    print_step("2/4", "æ¸¬è©¦ HF_TOKEN", timestamp=True)
    if not test_token():
        return 1
    print()

    # 3. æ¸¬è©¦æ¨¡å‹å­˜å–
    print_step("3/4", "æ¸¬è©¦æ¨¡å‹å­˜å–", timestamp=True)
    if not test_model_access():
        return 1
    print()

    # 4. æ¸¬è©¦ç¿»è­¯ï¼ˆå¯é¸ï¼Œå› ç‚ºè¼‰å…¥æ¨¡å‹éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰
    print(f"{Colors.YELLOW}{'='*80}{Colors.NC}")
    print(f"{Colors.YELLOW}æ­¥é©Ÿ 4/4: ç¿»è­¯åŠŸèƒ½æ¸¬è©¦ï¼ˆå¯é¸ï¼‰{Colors.NC}")
    print(f"{Colors.YELLOW}{'='*80}{Colors.NC}")
    print()

    # å…ˆæª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰
    MODEL_ID = os.getenv("MODEL_ID", "google/translategemma-4b-it")
    cache_status = check_model_cache(MODEL_ID)

    # æ ¹æ“šæ¨¡å‹ç‹€æ…‹é¡¯ç¤ºä¸åŒè¨Šæ¯
    if cache_status.get("cached"):
        # æ¨¡å‹å·²ä¸‹è¼‰
        print_success(f"æ¨¡å‹å·²åœ¨å¿«å–ä¸­ï¼ˆ{cache_status['size_gb']:.1f} GBï¼‰")
        print(f"   {Colors.CYAN}åªéœ€è¦è¼‰å…¥åˆ°è¨˜æ†¶é«”ï¼Œç„¡éœ€é‡æ–°ä¸‹è¼‰{Colors.NC}")
        required_disk = 3.0  # åªéœ€è¦è‡¨æ™‚ç©ºé–“
        required_mem = 10.0
        print(f"   å»ºè­°è‡³å°‘æœ‰ {Colors.BOLD}{required_disk:.0f} GB å¯ç”¨ç£ç¢Ÿç©ºé–“{Colors.NC}ï¼ˆè¼‰å…¥è‡¨æ™‚ç©ºé–“ï¼‰")
        print(f"   å’Œ {Colors.BOLD}{required_mem:.0f} GB å¯ç”¨è¨˜æ†¶é«”{Colors.NC}")
    else:
        # æ¨¡å‹æœªä¸‹è¼‰
        print_warning("æ¨¡å‹å°šæœªä¸‹è¼‰ï¼Œæ­¤æ­¥é©Ÿæœƒä¸‹è¼‰å®Œæ•´æ¨¡å‹ï¼ˆç´„ 8-9 GBï¼‰ä¸¦è¼‰å…¥åˆ°è¨˜æ†¶é«”")
        required_disk = 12.0  # éœ€è¦ä¸‹è¼‰ + è‡¨æ™‚ç©ºé–“
        required_mem = 10.0
        print(f"   å»ºè­°è‡³å°‘æœ‰ {Colors.BOLD}{required_disk:.0f} GB å¯ç”¨ç£ç¢Ÿç©ºé–“{Colors.NC}")
        print(f"   å’Œ {Colors.BOLD}{required_mem:.0f} GB å¯ç”¨è¨˜æ†¶é«”{Colors.NC}")

    print()

    # æª¢æŸ¥ç©ºé–“æ˜¯å¦è¶³å¤ ï¼ˆä½¿ç”¨å‹•æ…‹éœ€æ±‚ï¼‰
    free_disk_gb = disk.free / (1024**3)
    free_mem_gb = mem.available / (1024**3)

    if free_disk_gb < required_disk:
        print_warning(f"ç£ç¢Ÿç©ºé–“ä¸è¶³ï¼ˆåƒ… {free_disk_gb:.1f}GBï¼‰ï¼Œå¯èƒ½æœƒå¤±æ•—")
        print(f"   {Colors.CYAN}å»ºè­°åŸ·è¡Œ: ./run-examples.sh cleanup{Colors.NC}")
    if free_mem_gb < required_mem:
        print_warning(f"å¯ç”¨è¨˜æ†¶é«”ä¸è¶³ï¼ˆåƒ… {free_mem_gb:.1f}GBï¼‰ï¼Œå¯èƒ½æœƒå¤±æ•—")

    print()
    response = input("æ˜¯å¦åŸ·è¡Œç¿»è­¯æ¸¬è©¦ï¼Ÿ[y/N]: ")
    if response.lower() == 'y':
        if not test_translation():
            return 1
    else:
        print()
        print_warning("è·³éç¿»è­¯æ¸¬è©¦")

    print()
    print("="*80)
    print_success("æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
    print("="*80)
    print()
    print(f"{Colors.CYAN}ä¸‹ä¸€æ­¥ï¼š{Colors.NC}")
    print(f"   1. åœ¨ Colab ä¸­é–‹å•Ÿ translategemma-colab.ipynb")
    print(f"   2. æˆ–éƒ¨ç½²åˆ° Cloud Run: cd cloudrun && ./deploy.sh")
    print()

    return 0

if __name__ == "__main__":
    sys.exit(main())
