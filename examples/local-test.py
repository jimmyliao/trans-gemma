"""
æœ¬åœ°æ¸¬è©¦è…³æœ¬ - ä½¿ç”¨ .env æª”æ¡ˆå„²å­˜ HF_TOKEN

ä½¿ç”¨æ–¹å¼ï¼š
1. è¤‡è£½ .env.example åˆ° .env
2. åœ¨ .env ä¸­å¡«å…¥ä½ çš„ HF_TOKEN
3. åŸ·è¡Œ: python examples/local-test.py
"""

import os
import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def load_env():
    """è¼‰å…¥ .env æª”æ¡ˆ"""
    env_file = project_root / ".env"

    if not env_file.exists():
        print("âŒ .env æª”æ¡ˆä¸å­˜åœ¨")
        print()
        print("è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š")
        print("1. è¤‡è£½ .env.example åˆ° .env:")
        print("   cp .env.example .env")
        print()
        print("2. ç·¨è¼¯ .env ä¸¦å¡«å…¥ä½ çš„ HF_TOKEN:")
        print("   # å¾ https://huggingface.co/settings/tokens å–å¾—")
        print("   HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
        return False

    # è®€å– .env æª”æ¡ˆ
    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

    return True

def test_token():
    """æ¸¬è©¦ HF_TOKEN æ˜¯å¦æœ‰æ•ˆ"""
    token = os.getenv("HF_TOKEN")

    if not token or token.startswith("hf_xxx"):
        print("âŒ HF_TOKEN æœªè¨­å®šæˆ–ä½¿ç”¨é è¨­å€¼")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæœ‰æ•ˆçš„ HF_TOKEN")
        return False

    print("âœ… HF_TOKEN å·²è¨­å®š")
    print(f"   Token: {token[:10]}...{token[-5:]}")

    try:
        from huggingface_hub import login
        login(token=token)
        print("âœ… Hugging Face èªè­‰æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ Hugging Face èªè­‰å¤±æ•—: {e}")
        return False

def test_model_access():
    """æ¸¬è©¦æ¨¡å‹å­˜å–"""
    print("\næ¸¬è©¦ TranslateGemma æ¨¡å‹å­˜å–...")

    try:
        from transformers import AutoTokenizer

        MODEL_ID = os.getenv("MODEL_ID", "google/translategemma-4b-it")
        print(f"è¼‰å…¥ tokenizer: {MODEL_ID}")

        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        print(f"âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
        print(f"   è©å½™è¡¨å¤§å°: {len(tokenizer)}")
        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹å­˜å–å¤±æ•—: {e}")
        print()
        print("å¯èƒ½çš„åŸå› ï¼š")
        print("1. å°šæœªç”³è«‹ TranslateGemma å­˜å–æ¬Šé™")
        print("   å‰å¾€: https://huggingface.co/google/translategemma-4b-it")
        print("2. Token æ¬Šé™ä¸è¶³ï¼ˆéœ€è¦ Read æ¬Šé™ï¼‰")
        return False

def test_translation():
    """æ¸¬è©¦ç¿»è­¯åŠŸèƒ½"""
    print("\næ¸¬è©¦ç¿»è­¯åŠŸèƒ½...")

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        MODEL_ID = os.getenv("MODEL_ID", "google/translategemma-4b-it")

        print("è¼‰å…¥æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )

        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")

        # æ¸¬è©¦ç¿»è­¯ï¼ˆä½¿ç”¨æ­£ç¢ºçš„ TranslateGemma æ ¼å¼ï¼‰
        print("\nåŸ·è¡Œæ¸¬è©¦ç¿»è­¯...")
        text = "Hello, world!"

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": text,
                        "source_lang_code": "eng",
                        "target_lang_code": "zho_Hant"
                    }
                ]
            }
        ]

        inputs = tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            add_generation_prompt=True
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=128,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print(f"åŸæ–‡: {text}")
        print(f"è­¯æ–‡: {result}")
        print()
        print("âœ… ç¿»è­¯æ¸¬è©¦æˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ ç¿»è­¯æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("="*80)
    print("TranslateGemma æœ¬åœ°æ¸¬è©¦")
    print("="*80)
    print()

    # 1. è¼‰å…¥ .env
    print("æ­¥é©Ÿ 1: è¼‰å…¥ .env æª”æ¡ˆ")
    if not load_env():
        return 1
    print("âœ… .env æª”æ¡ˆè¼‰å…¥æˆåŠŸ")
    print()

    # 2. æ¸¬è©¦ token
    print("æ­¥é©Ÿ 2: æ¸¬è©¦ HF_TOKEN")
    if not test_token():
        return 1
    print()

    # 3. æ¸¬è©¦æ¨¡å‹å­˜å–
    print("æ­¥é©Ÿ 3: æ¸¬è©¦æ¨¡å‹å­˜å–")
    if not test_model_access():
        return 1
    print()

    # 4. æ¸¬è©¦ç¿»è­¯ï¼ˆå¯é¸ï¼Œå› ç‚ºè¼‰å…¥æ¨¡å‹éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰
    print("æ­¥é©Ÿ 4: æ¸¬è©¦ç¿»è­¯åŠŸèƒ½")
    response = input("æ˜¯å¦åŸ·è¡Œç¿»è­¯æ¸¬è©¦ï¼Ÿ(è¼‰å…¥æ¨¡å‹éœ€è¦è¼ƒé•·æ™‚é–“) [y/N]: ")
    if response.lower() == 'y':
        if not test_translation():
            return 1
    else:
        print("â­ï¸  è·³éç¿»è­¯æ¸¬è©¦")

    print()
    print("="*80)
    print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
    print("="*80)
    print()
    print("ä¸‹ä¸€æ­¥ï¼š")
    print("1. åœ¨ Colab ä¸­é–‹å•Ÿ translategemma-colab.ipynb")
    print("2. æˆ–éƒ¨ç½²åˆ° Cloud Run: cd cloudrun && ./deploy.sh")

    return 0

if __name__ == "__main__":
    sys.exit(main())
