# TranslateGemma å®Œå…¨å¯¦æˆ°æŒ‡å—ï¼šå¾ Google Colab åˆ°å¤šæ¨¡æ…‹ç¿»è­¯

> ä½¿ç”¨ Google æœ€æ–°é–‹æºç¿»è­¯æ¨¡å‹ TranslateGemmaï¼Œåœ¨ Colab å…è²» GPU ä¸Šå¯¦ç¾ PDFã€åœ–ç‰‡ã€ç¶²é çš„ç¹é«”ä¸­æ–‡ç¿»è­¯

**ä½œè€…**ï¼šJimmy Liao (AI GDE)
**ç™¼å¸ƒæ—¥æœŸ**ï¼š2026-01-17
**æ¨™ç±¤**ï¼š`TranslateGemma` `Google Colab` `å¤šæ¨¡æ…‹ç¿»è­¯` `ç¹é«”ä¸­æ–‡` `Python`

---

## å‰è¨€ï¼šç‚ºä»€éº¼é¸æ“‡ TranslateGemmaï¼Ÿ

ä½œç‚ºé–‹ç™¼è€…ï¼Œæˆ‘å€‘ç¶“å¸¸é‡åˆ°éœ€è¦ç¿»è­¯æŠ€è¡“æ–‡ä»¶ã€ç ”ç©¶è«–æ–‡æˆ–å¤–èªç¶²ç«™çš„æƒ…æ³ã€‚å‚³çµ±çš„ç¿»è­¯æœå‹™é›–ç„¶æ–¹ä¾¿ï¼Œä½†å¾€å¾€é¢è‡¨ä»¥ä¸‹å•é¡Œï¼š

- **éš±ç§ç–‘æ…®**ï¼šæ•æ„Ÿæ–‡ä»¶ä¸é©åˆä¸Šå‚³åˆ°é›²ç«¯æœå‹™
- **æˆæœ¬è€ƒé‡**ï¼šå¤§é‡ç¿»è­¯éœ€æ±‚æœƒç”¢ç”Ÿå¯è§€è²»ç”¨
- **å®¢è£½åŒ–é™åˆ¶**ï¼šç„¡æ³•é‡å°å°ˆæ¥­è¡“èªé€²è¡Œå¾®èª¿
- **é›¢ç·šéœ€æ±‚**ï¼šåœ¨æ²’æœ‰ç¶²è·¯çš„ç’°å¢ƒç„¡æ³•ä½¿ç”¨

Google åœ¨ 2025 å¹´ç™¼å¸ƒçš„ **TranslateGemma** ç³»åˆ—æ¨¡å‹å®Œç¾è§£æ±ºäº†é€™äº›ç—›é»ï¼š

âœ… **é–‹æºå…è²»**ï¼šå•†ç”¨æˆæ¬Šï¼Œå¯è‡ªç”±éƒ¨ç½²
âœ… **å¤šæ¨¡æ…‹èƒ½åŠ›**ï¼šæ”¯æ´æ–‡å­— + åœ–ç‰‡åŒæ™‚è™•ç†
âœ… **å¤šèªè¨€æ”¯æ´**ï¼š55 ç¨®èªè¨€ï¼ŒåŒ…å«ç¹é«”ä¸­æ–‡
âœ… **æœ¬åœ°é‹è¡Œ**ï¼šå¯åœ¨å€‹äººé›»è…¦æˆ– Colab åŸ·è¡Œ
âœ… **é«˜å“è³ªè¼¸å‡º**ï¼šåŸºæ–¼ Gemini æ¶æ§‹è¨“ç·´

æœ¬æ–‡å°‡å¸¶ä½ å¾é›¶é–‹å§‹ï¼Œä½¿ç”¨ Google Colab å…è²» GPU ç’°å¢ƒï¼Œæ‰“é€ ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„ç¿»è­¯ç³»çµ±ã€‚

---

## ä¸€ã€ç’°å¢ƒæº–å‚™ï¼šç‚ºä½•é¸æ“‡ Google Colabï¼Ÿ

### 1.1 Colab-First ç­–ç•¥

ç›¸æ¯”æœ¬åœ°é–‹ç™¼ï¼Œä½¿ç”¨ Google Colab æœ‰ä»¥ä¸‹å„ªå‹¢ï¼š

| ç‰¹æ€§ | æœ¬åœ°ç’°å¢ƒ | Google Colab |
|------|---------|-------------|
| **GPU æˆæœ¬** | éœ€è³¼è²·æˆ–ç§Ÿç”¨ | å…è²» T4 GPU |
| **ç’°å¢ƒé…ç½®** | è¤‡é›œä¾è³´å®‰è£ | é è£ CUDA/PyTorch |
| **å„²å­˜ç©ºé–“** | å—é™æ–¼æœ¬æ©Ÿ | éœ€æ±‚æ™‚ä¸‹è¼‰æ¨¡å‹ |
| **å”ä½œåˆ†äº«** | å›°é›£ | ä¸€éµåˆ†äº« notebook |
| **ç¶²è·¯é€Ÿåº¦** | ä¾è³´æœ¬åœ°ç¶²è·¯ | Google æ©Ÿæˆ¿é«˜é€Ÿä¸‹è¼‰ |

### 1.2 å°ˆæ¡ˆçµæ§‹

æˆ‘å€‘æ¡ç”¨ **Single Source of Truth** è¨­è¨ˆï¼šæ‰€æœ‰ç¨‹å¼ç¢¼çµ±ä¸€æ”¾åœ¨ GitHubï¼ŒColab notebook ç›´æ¥ clone repositoryã€‚

```bash
trans-gemma/
â”œâ”€â”€ document-translator-colab.ipynb  # ä¸»è¦ Notebook
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ translate.py                 # CLI ç¿»è­¯å·¥å…·
â”‚   â””â”€â”€ backends/
â”‚       â”œâ”€â”€ transformers_backend.py          # æ–‡å­—ç¿»è­¯
â”‚       â”œâ”€â”€ transformers_multimodal_backend.py  # å¤šæ¨¡æ…‹ç¿»è­¯
â”‚       â”œâ”€â”€ ollama_backend.py        # Ollama æœ¬åœ°æ¨ç†
â”‚       â””â”€â”€ mlx_backend.py           # Apple Silicon å„ªåŒ–
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

### 1.3 å¿«é€Ÿé–‹å§‹

åœ¨ Colab ä¸­é–‹å•Ÿ notebookï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jimmyliao/trans-gemma/blob/main/document-translator-colab.ipynb)

åŸ·è¡Œå‰å…©å€‹ cellï¼š

```python
# Cell 1: Clone å°ˆæ¡ˆ
!rm -rf trans-gemma
!git clone https://github.com/jimmyliao/trans-gemma.git
%cd trans-gemma

# Cell 2: å®‰è£ä¾è³´
!pip install uv -q
!uv pip install --system -e ".[examples]"
```

---

## äºŒã€èªè­‰è¨­å®šï¼šå­˜å– Gated Model

TranslateGemma æ˜¯ **gated model**ï¼Œéœ€è¦å…ˆå–å¾—æˆæ¬Šã€‚

### 2.1 å–å¾— HuggingFace Token

1. å‰å¾€ [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. å»ºç«‹æ–° tokenï¼ˆéœ€è¦ `read` æ¬Šé™ï¼‰
3. å‰å¾€ [TranslateGemma æ¨¡å‹é ](https://huggingface.co/google/translategemma-4b-it) æ¥å—æˆæ¬Š

### 2.2 ä½¿ç”¨ Colab Secretsï¼ˆæ¨è–¦ï¼‰

åœ¨ Colab å·¦å´æ¬„é»æ“Š ğŸ”‘ åœ–ç¤ºï¼Œæ–°å¢ secretï¼š

- **Name**: `HF_TOKEN`
- **Value**: ä½ çš„ HuggingFace token

```python
from huggingface_hub import login
from google.colab import userdata

HF_TOKEN = userdata.get('HF_TOKEN')
login(token=HF_TOKEN)
```

**ç‚ºä½•ä½¿ç”¨ Secretsï¼Ÿ**
- âœ… ä¸æœƒå°‡ token å¯«å…¥ notebook ç¨‹å¼ç¢¼
- âœ… åˆ†äº« notebook æ™‚ä¸æœƒæ´©æ¼æ†‘è­‰
- âœ… ç¬¦åˆå®‰å…¨æœ€ä½³å¯¦è¸

---

## ä¸‰ã€å…­ç¨®ç¿»è­¯æ¨¡å¼è©³è§£

### 3.1 Mode 1: arXiv è«–æ–‡è‡ªå‹•ä¸‹è¼‰ç¿»è­¯

**ä½¿ç”¨å ´æ™¯**ï¼šå¿«é€Ÿé–±è®€æœ€æ–°ç ”ç©¶è«–æ–‡

```python
# é…ç½®
ARXIV_ID = "2601.09012v2"  # TranslateGemma æŠ€è¡“å ±å‘Š
START_PAGE = 1
END_PAGE = 1
TARGET_LANG = "zh-TW"

# è‡ªå‹•ä¸‹è¼‰ä¸¦ç¿»è­¯
!python examples/translate.py \
  --mode pdf \
  --arxiv {ARXIV_ID} \
  --backend transformers \
  --target {TARGET_LANG} \
  --start-page {START_PAGE} \
  --end-page {END_PAGE}
```

**æŠ€è¡“ç´°ç¯€**ï¼š
- ä½¿ç”¨ `arxiv` Python package è‡ªå‹•ä¸‹è¼‰ PDF
- æ”¯æ´ç‰ˆæœ¬è™ŸæŒ‡å®šï¼ˆå¦‚ `v2`ï¼‰
- è‡ªå‹•è§£æ PDF æ–‡å­—å…§å®¹
- é€é ç¿»è­¯ä¸¦è¼¸å‡º

### 3.2 Mode 2: ä¸Šå‚³ PDF ç¿»è­¯

**ä½¿ç”¨å ´æ™¯**ï¼šç¿»è­¯æœ¬åœ°æ–‡ä»¶ã€åˆç´„ã€æŠ€è¡“æ‰‹å†Š

```python
from google.colab import files

# ä¸Šå‚³ PDF
uploaded = files.upload()
pdf_file = list(uploaded.keys())[0]

# ç¿»è­¯è¨­å®š
START_PAGE = 1
END_PAGE = 3
USE_IMAGE_MODE = False  # æ–‡å­—æ¨¡å¼è¼ƒå¿«

!python examples/translate.py \
  --mode pdf \
  --file {pdf_file} \
  --backend transformers \
  --target zh-TW \
  --start-page {START_PAGE} \
  --end-page {END_PAGE}
```

### 3.3 Mode 3: PDF åœ–ç‰‡æ¨¡å¼ï¼ˆä¿ç•™ç‰ˆé¢ï¼‰

**ä½¿ç”¨å ´æ™¯**ï¼šåŒ…å«åœ–è¡¨ã€å…¬å¼ã€è¤‡é›œæ’ç‰ˆçš„æ–‡ä»¶

```python
# å¤šæ¨¡æ…‹ç¿»è­¯ - ä¿ç•™è¦–è¦ºä¸Šä¸‹æ–‡
ARXIV_ID = "2601.09012v2"
START_PAGE = 3  # æœ‰åœ–è¡¨çš„é é¢
DPI = 96  # è§£æåº¦ï¼ˆè¶Šé«˜è¶Šæ…¢ä½†è¶Šæ¸…æ™°ï¼‰

!python examples/translate.py \
  --mode pdf \
  --arxiv {ARXIV_ID} \
  --backend transformers \
  --target zh-TW \
  --pdf-as-image \  # é—œéµï¼šå•Ÿç”¨åœ–ç‰‡æ¨¡å¼
  --dpi {DPI} \
  --start-page {START_PAGE} \
  --end-page {END_PAGE}
```

**èƒŒå¾ŒåŸç†**ï¼š
1. å°‡ PDF é é¢è½‰æ›ç‚ºåœ–ç‰‡ï¼ˆä½¿ç”¨ `pdf2image`ï¼‰
2. è¼‰å…¥ **å¤šæ¨¡æ…‹æ¨¡å‹**ï¼ˆ`AutoModelForImageTextToText`ï¼‰
3. åŒæ™‚è™•ç†åœ–ç‰‡ + èªè¨€ä»£ç¢¼
4. æ¨¡å‹ç†è§£è¦–è¦ºä¸Šä¸‹æ–‡å¾Œè¼¸å‡ºç¿»è­¯

**æ•ˆèƒ½æ¯”è¼ƒ**ï¼š

| æ¨¡å¼ | é€Ÿåº¦ | æº–ç¢ºåº¦ | é©ç”¨å ´æ™¯ |
|------|------|--------|---------|
| æ–‡å­—æ¨¡å¼ | âš¡âš¡âš¡ | âœ… | ç´”æ–‡å­— PDF |
| åœ–ç‰‡æ¨¡å¼ DPI=96 | âš¡âš¡ | âœ…âœ… | åŒ…å«åœ–è¡¨çš„æ–‡ä»¶ |
| åœ–ç‰‡æ¨¡å¼ DPI=150 | âš¡ | âœ…âœ…âœ… | è¤‡é›œæ’ç‰ˆã€OCRéœ€æ±‚ |

### 3.4 Mode 4: å–®å¼µåœ–ç‰‡ç¿»è­¯

**ä½¿ç”¨å ´æ™¯**ï¼šç¿»è­¯èœå–®ã€æµ·å ±ã€ç¤¾ç¾¤åª’é«”åœ–ç‰‡

```python
import urllib.request
import sys
sys.path.insert(0, 'examples/backends')

# ä¸‹è¼‰ç¤ºç¯„åœ–ç‰‡ï¼ˆæ—¥æ–‡èœå–®ï¼‰
image_url = "https://cdn.odigo.net/f91b9c108a1e0cd1117e1c46ee36eeca.jpg"
urllib.request.urlretrieve(image_url, "menu.jpg")

# è¼‰å…¥å¤šæ¨¡æ…‹å¾Œç«¯
from transformers_multimodal_backend import TransformersMultimodalBackend

backend = TransformersMultimodalBackend()
backend.load_model()

# ç¿»è­¯
result = backend.translate_image(
    "menu.jpg",
    source_lang="ja",
    target_lang="zh-TW"
)

print(result['translation'])
# è¼¸å‡ºï¼šèœå–®å…§å®¹çš„ç¹é«”ä¸­æ–‡ç¿»è­¯
```

**é—œéµå¯¦ä½œç´°ç¯€**ï¼š

```python
# transformers_multimodal_backend.py æ ¸å¿ƒé‚è¼¯
def translate_image(self, image_path, source_lang, target_lang):
    # 1. è¼‰å…¥åœ–ç‰‡
    image = Image.open(image_path).convert("RGB")

    # 2. å»ºæ§‹çµæ§‹åŒ–è¨Šæ¯ï¼ˆé‡è¦ï¼ï¼‰
    messages = [{
        "role": "user",
        "content": [{
            "type": "image",
            "image": image
        }, {
            "type": "text",
            "text": "",
            "source_lang_code": source_lang,  # ISO 639-1 ä»£ç¢¼
            "target_lang_code": target_lang   # å¦‚ "zh-TW"
        }]
    }]

    # 3. æ‡‰ç”¨ chat template
    inputs = self.processor.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to(self.device)

    # 4. ç”Ÿæˆç¿»è­¯
    outputs = self.model.generate(inputs, max_new_tokens=256)
    translation = self.processor.decode(outputs[0])

    # 5. å¾Œè™•ç†ï¼šç°¡è½‰ç¹
    if target_lang == "zh-TW":
        from hanziconv import HanziConv
        translation = HanziConv.toTraditional(translation)

    return translation
```

### 3.5 Mode 5: ç¶²é æ–‡ç« ç¿»è­¯ï¼ˆWeb Scrapingï¼‰â­ æ¨è–¦

**ä½¿ç”¨å ´æ™¯**ï¼šæŠ€è¡“éƒ¨è½æ ¼ã€æ–°èæ–‡ç« ã€æ–‡æª”ç¶²ç«™

**ç‚ºä½•æ¯”æˆªåœ–æ›´å¥½ï¼Ÿ**
- âœ… ç›´æ¥æå– HTML æ–‡å­—ï¼Œç„¡ OCR èª¤å·®
- âœ… é€Ÿåº¦å¿« 3-5 å€ï¼ˆç„¡éœ€æˆªåœ– + åœ–ç‰‡è™•ç†ï¼‰
- âœ… æ›´æº–ç¢ºï¼ˆä¿ç•™åŸå§‹æ–‡å­—ç·¨ç¢¼ï¼‰
- âœ… å¯æ“·å–æ›´å¤šå…§å®¹ï¼ˆä¸å—è¢å¹•é«˜åº¦é™åˆ¶ï¼‰

```python
import requests
from bs4 import BeautifulSoup

ARTICLE_URL = "https://aismiley.co.jp/ai_news/gemma3-rag-api-local-use/"

def extract_article_text(url):
    # 1. æŠ“å–ç¶²é 
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 2. ç§»é™¤é›œè¨Š
    for element in soup.select('nav, aside, footer, script, style'):
        element.decompose()

    # 3. æ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ
    content_selectors = ['main', 'article', '.content']
    content_area = None
    for selector in content_selectors:
        content_area = soup.select_one(selector)
        if content_area and len(content_area.find_all('p')) > 3:
            break

    # 4. æå–æ®µè½
    paragraphs = []
    seen_texts = set()

    for element in content_area.find_all(['p', 'h2', 'h3', 'li']):
        text = element.get_text(strip=True)
        if len(text) > 15 and text not in seen_texts:
            seen_texts.add(text)
            paragraphs.append(text)

    # 5. çµ„åˆæ–‡å­—ï¼ˆé™åˆ¶ 20 æ®µé¿å…è¶…é token é™åˆ¶ï¼‰
    title = soup.find('h1').get_text(strip=True)
    full_text = f"{title}\n\n" + "\n\n".join(paragraphs[:20])

    return full_text

# æå–ä¸¦ç¿»è­¯
article_text = extract_article_text(ARTICLE_URL)

from transformers_backend import TransformersBackend
backend = TransformersBackend()
backend.load_model()

result = backend.translate(article_text, source_lang="ja", target_lang="zh-TW")
print(result['translation'])
```

**å¯¦æˆ°æŠ€å·§**ï¼š

1. **è™•ç†ä¸åŒç¶²ç«™çµæ§‹**ï¼š
   ```python
   # ç­–ç•¥ 1: å˜—è©¦å¸¸è¦‹é¸æ“‡å™¨
   selectors = ['main', 'article', '.post-content', '.entry-content']

   # ç­–ç•¥ 2: æ‰¾æ®µè½æ•¸é‡æœ€å¤šçš„å€åŸŸ
   max_paragraphs = 0
   best_container = None
   for container in soup.find_all(['div', 'section']):
       p_count = len(container.find_all('p'))
       if p_count > max_paragraphs:
           max_paragraphs = p_count
           best_container = container
   ```

2. **éæ¿¾é›œè¨Šå…§å®¹**ï¼š
   ```python
   # è·³éå°èˆªã€å»£å‘Šã€æ³•å¾‹è²æ˜
   skip_patterns = [
       'cookie', 'privacy', 'terms',
       'åˆ©ç”¨è¦ç´„', 'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼'
   ]

   if any(pattern in text.lower() for pattern in skip_patterns):
       continue
   ```

3. **é¿å…é‡è¤‡**ï¼š
   ```python
   seen_texts = set()
   if text not in seen_texts:
       seen_texts.add(text)
       paragraphs.append(text)
   ```

### 3.6 Mode 6: ç¶²é æˆªåœ–ç¿»è­¯

**ä½¿ç”¨å ´æ™¯**ï¼šå‹•æ…‹ç¶²é ã€éœ€è¦è¦–è¦ºä¸Šä¸‹æ–‡çš„é é¢

```python
from playwright.async_api import async_playwright

WEBSITE_URL = "https://www.yomiuri.co.jp/national/20260117-GYT1T00119/"

async def capture_screenshot(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page(viewport={'width': 1280, 'height': 1024})
        await page.goto(url, wait_until='networkidle')
        await page.screenshot(path='screenshot.png', full_page=False)
        await browser.close()

# æˆªåœ–
await capture_screenshot(WEBSITE_URL)

# ç¿»è­¯æˆªåœ–
from transformers_multimodal_backend import TransformersMultimodalBackend
backend = TransformersMultimodalBackend()
backend.load_model()

result = backend.translate_image('screenshot.png', source_lang="ja", target_lang="zh-TW")
```

**Colab ç‰¹æ®Šè™•ç†**ï¼š

Colab ç’°å¢ƒä½¿ç”¨ asyncio event loopï¼Œå¿…é ˆä½¿ç”¨ **async API**ï¼š

```python
# âŒ éŒ¯èª¤ï¼šæœƒå ±éŒ¯ "Playwright Sync API inside asyncio loop"
from playwright.sync_api import sync_playwright

# âœ… æ­£ç¢ºï¼šä½¿ç”¨ async API
from playwright.async_api import async_playwright
await capture_screenshot(url)  # åœ¨ Colab ä¸­å¯ç›´æ¥ await
```

éœ€è¦å®‰è£ç³»çµ±ä¾è³´ï¼š

```bash
!apt-get install -y -qq libatk1.0-0 libatk-bridge2.0-0 libcups2 \
  libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 \
  libpango-1.0-0 libcairo2 libasound2

!playwright install chromium --with-deps
```

---

## å››ã€æ ¸å¿ƒæŠ€è¡“å‰–æ

### 4.1 ç¹é«”ä¸­æ–‡å¼·åˆ¶è¼¸å‡ºçš„å¯¦ç¾

**å•é¡Œ**ï¼šTranslateGemma é è¨­å¯èƒ½è¼¸å‡ºç°¡é«”ä¸­æ–‡ï¼ˆè®­ç»ƒæ•°æ®ä¸­ç®€ä½“å æ¯”æ›´é«˜ï¼‰

**è§£æ±ºæ–¹æ¡ˆ**ï¼šå¤šå±¤æ¬¡ç¢ºä¿ç¹é«”è¼¸å‡º

#### ç­–ç•¥ 1: æ­£ç¢ºçš„èªè¨€ä»£ç¢¼

```python
# âŒ éŒ¯èª¤ï¼šä½¿ç”¨ ISO 639-3
messages = [{
    "content": [{
        "source_lang_code": "eng",      # âŒ
        "target_lang_code": "zho_Hant"  # âŒ
    }]
}]

# âœ… æ­£ç¢ºï¼šä½¿ç”¨ ISO 639-1
messages = [{
    "content": [{
        "source_lang_code": "en",     # âœ…
        "target_lang_code": "zh-TW"   # âœ… æ˜ç¢ºæŒ‡å®šå°ç£ç¹é«”
    }]
}]
```

#### ç­–ç•¥ 2: å¾Œè™•ç†è½‰æ›ï¼ˆä¿éšªæ©Ÿåˆ¶ï¼‰

```python
# transformers_backend.py
def translate(self, text, source_lang, target_lang):
    # ... æ¨¡å‹æ¨ç† ...

    # å¾Œè™•ç†ï¼šç¢ºä¿è¼¸å‡ºç¹é«”ä¸­æ–‡
    if target_lang == "zh-TW":
        try:
            from hanziconv import HanziConv
            translation = HanziConv.toTraditional(translation)
        except ImportError:
            pass  # hanziconv æœªå®‰è£æ™‚è·³é

    return translation
```

**ç‚ºä½•ä½¿ç”¨ hanziconvï¼Ÿ**
- âœ… è¼•é‡ç´šï¼ˆ~500KBï¼‰
- âœ… æº–ç¢ºåº¦é«˜ï¼ˆåŸºæ–¼ OpenCCï¼‰
- âœ… ç„¡å¤–éƒ¨ä¾è³´
- âœ… æ¯” `opencc-python-reimplemented` æ›´å¿«

#### ç­–ç•¥ 3: Chat Template é©—è­‰

ç¢ºä¿ tokenizer æ­£ç¢ºæ‡‰ç”¨èªè¨€ä»£ç¢¼ï¼š

```python
inputs = tokenizer.apply_chat_template(
    messages,
    return_tensors="pt",
    add_generation_prompt=True
)

# æª¢æŸ¥ tokenized çµæœæ˜¯å¦åŒ…å«æ­£ç¢ºçš„èªè¨€ token
print(tokenizer.decode(inputs[0][:50]))
# æ‡‰çœ‹åˆ°é¡ä¼¼ï¼š<start_of_turn>user\nzh-TW<end_of_turn>...
```

### 4.2 å¤šæ¨¡æ…‹æ¨¡å‹çš„å·¥ä½œåŸç†

TranslateGemma å¤šæ¨¡æ…‹ç‰ˆæœ¬åŸºæ–¼ **Gemma 2 æ¶æ§‹** + **Vision Encoder**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image     â”‚
â”‚  (RGB åœ–ç‰‡)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision Encoder â”‚  â† SigLIP (é¡ä¼¼ CLIP)
â”‚  (æå–è¦–è¦ºç‰¹å¾µ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vision-Language â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  Text Input  â”‚
â”‚   Projector     â”‚         â”‚ (èªè¨€ä»£ç¢¼)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemma 2 LLM    â”‚  â† 4B åƒæ•¸çš„èªè¨€æ¨¡å‹
â”‚  (ç”Ÿæˆç¿»è­¯æ–‡å­—)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Translation   â”‚
â”‚   (ç¹é«”ä¸­æ–‡)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é—œéµæŠ€è¡“é»**ï¼š

1. **Image Patches**: åœ–ç‰‡åˆ‡åˆ†ç‚º 14Ã—14 patches
2. **Vision-Language Alignment**: è¦–è¦ºç‰¹å¾µå°é½Šåˆ°èªè¨€ç©ºé–“
3. **Context Window**: æ”¯æ´ 128K tokensï¼ˆå¯è™•ç†é•·æ–‡ç« ï¼‰
4. **Streaming Generation**: æ”¯æ´ä¸²æµè¼¸å‡ºï¼ˆé©åˆ UI é¡¯ç¤ºï¼‰

### 4.3 Backend æ¶æ§‹è¨­è¨ˆ

æ¡ç”¨ **Strategy Pattern** æ”¯æ´å¤šç¨®æ¨ç†å¾Œç«¯ï¼š

```python
# examples/backends/base.py
class TranslationBackend(ABC):
    def __init__(self):
        self.model_id = "google/translategemma-4b-it"
        self.model = None
        self.tokenizer = None

    @abstractmethod
    def load_model(self, **kwargs) -> Dict[str, Any]:
        """è¼‰å…¥æ¨¡å‹"""
        pass

    @abstractmethod
    def translate(self, text: str, source_lang: str, target_lang: str) -> Dict[str, Any]:
        """åŸ·è¡Œç¿»è­¯"""
        pass

# å…·é«”å¯¦ä½œ
class TransformersBackend(TranslationBackend):
    """HuggingFace Transformers å¾Œç«¯"""

class OllamaBackend(TranslationBackend):
    """Ollama æœ¬åœ°æ¨ç†å¾Œç«¯"""

class MLXBackend(TranslationBackend):
    """Apple Silicon å„ªåŒ–å¾Œç«¯"""
```

**Factory Pattern**ï¼š

```python
def get_backend(name='transformers'):
    backends = {
        'transformers': TransformersBackend,
        'ollama': OllamaBackend,
        'mlx': MLXBackend
    }
    return backends[name]()
```

**ç‚ºä½•é€™æ¨£è¨­è¨ˆï¼Ÿ**
- âœ… å¯æ“´å±•ï¼šæ–°å¢å¾Œç«¯åªéœ€ç¹¼æ‰¿ base class
- âœ… å¯æ¸¬è©¦ï¼šæ¯å€‹ backend ç¨ç«‹æ¸¬è©¦
- âœ… å¯æ›¿æ›ï¼šæ ¹æ“šç’°å¢ƒé¸æ“‡æœ€ä½³å¾Œç«¯
- âœ… ä¸€è‡´ä»‹é¢ï¼šä½¿ç”¨è€…ç„¡éœ€é—œå¿ƒåº•å±¤å¯¦ä½œ

---

## äº”ã€æ•ˆèƒ½å„ªåŒ–èˆ‡æœ€ä½³å¯¦è¸

### 5.1 è¨˜æ†¶é«”ç®¡ç†

**å•é¡Œ**ï¼šColab å…è²»ç‰ˆè¨˜æ†¶é«”æœ‰é™ï¼ˆ~12GBï¼‰

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

```python
# 1. ä½¿ç”¨ bfloat16 é™ä½è¨˜æ†¶é«”ä½¿ç”¨
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,  # æ¯” float32 çœä¸€åŠè¨˜æ†¶é«”
    device_map="auto"
)

# 2. é™åˆ¶æœ€å¤§è¨˜æ†¶é«”
load_kwargs = {
    "max_memory": {"0": "8GiB", "cpu": "8GiB"}
}

# 3. ç¿»è­¯å¾Œæ¸…ç†
def cleanup():
    del model
    del tokenizer
    torch.cuda.empty_cache()
```

### 5.2 æ‰¹æ¬¡ç¿»è­¯ç­–ç•¥

**å–®æ¬¡ç¿»è­¯ vs æ‰¹æ¬¡ç¿»è­¯**ï¼š

```python
# âŒ ä¸æ•ˆç‡ï¼šæ¯æ®µéƒ½é‡æ–°è¼‰å…¥æ¨¡å‹
for paragraph in paragraphs:
    backend = TransformersBackend()
    backend.load_model()  # è¼‰å…¥æ™‚é–“ ~10s
    result = backend.translate(paragraph)

# âœ… æ•ˆç‡ï¼šè¼‰å…¥ä¸€æ¬¡ï¼Œç¿»è­¯å¤šæ®µ
backend = TransformersBackend()
backend.load_model()  # åªè¼‰å…¥ä¸€æ¬¡

for paragraph in paragraphs:
    result = backend.translate(paragraph)  # æ¯æ¬¡ ~2-5s
```

**Token é™åˆ¶è™•ç†**ï¼š

```python
def split_text_by_tokens(text, max_tokens=512):
    """å°‡é•·æ–‡æœ¬åˆ†æ®µï¼Œé¿å…è¶…éæ¨¡å‹é™åˆ¶"""
    sentences = text.split('ã€‚')
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sentence_tokens = len(tokenizer.encode(sentence))
        if current_length + sentence_tokens > max_tokens:
            chunks.append('ã€‚'.join(current_chunk) + 'ã€‚')
            current_chunk = [sentence]
            current_length = sentence_tokens
        else:
            current_chunk.append(sentence)
            current_length += sentence_tokens

    if current_chunk:
        chunks.append('ã€‚'.join(current_chunk))

    return chunks
```

### 5.3 é€Ÿåº¦æ¯”è¼ƒ

åœ¨ Colab T4 GPU ä¸Šçš„å¯¦æ¸¬æ•¸æ“šï¼š

| ä»»å‹™é¡å‹ | æ¨¡å¼ | æ™‚é–“ | Tokens/s | å‚™è¨» |
|---------|------|------|----------|------|
| ç´”æ–‡å­—ç¿»è­¯ | transformers | ~20s | 17.2 | 256 tokens è¼¸å…¥ |
| å–®å¼µåœ–ç‰‡ | multimodal | ~15s | 12.5 | æ—¥æ–‡èœå–® |
| PDF æ–‡å­—æ¨¡å¼ | transformers | ~25s/é  | 15.8 | A4 é é¢ |
| PDF åœ–ç‰‡æ¨¡å¼ DPI=96 | multimodal | ~40s/é  | 8.3 | åŒ…å«åœ–è¡¨ |
| ç¶²é æŠ“å– | transformers | ~22s | 16.9 | 20 æ®µè½ |
| ç¶²é æˆªåœ– | multimodal | ~18s | 11.2 | 1280Ã—1024 |

**å„ªåŒ–å»ºè­°**ï¼š
- ç´”æ–‡å­—å…§å®¹ â†’ å„ªå…ˆä½¿ç”¨æ–‡å­—æ¨¡å¼
- æœ‰åœ–è¡¨/å…¬å¼ â†’ ä½¿ç”¨åœ–ç‰‡æ¨¡å¼
- ç¶²é å…§å®¹ â†’ Web Scraping å„ªå…ˆï¼ˆæ›´å¿«æ›´æº–ï¼‰
- å‹•æ…‹ç¶²é  â†’ Screenshot æ¨¡å¼

---

## å…­ã€å¸¸è¦‹å•é¡Œæ’æŸ¥

### 6.1 èªè­‰éŒ¯èª¤

```
huggingface_hub.errors.GatedRepoError: 401 Client Error
```

**è§£æ±ºæ–¹æ³•**ï¼š
1. ç¢ºèªå·²åœ¨ HuggingFace æ¥å—æ¨¡å‹æˆæ¬Š
2. Token æ¬Šé™åŒ…å« `read`
3. Colab Secrets åç¨±æ­£ç¢ºï¼ˆ`HF_TOKEN`ï¼‰

### 6.2 Import éŒ¯èª¤

```
ModuleNotFoundError: No module named 'backends'
```

**åŸå› **ï¼šColab ç’°å¢ƒçš„ç›¸å° import å•é¡Œ

**è§£æ±ºæ–¹æ³•**ï¼š

```python
# åœ¨æ¯å€‹ backend æª”æ¡ˆåŠ å…¥ fallback
try:
    from .base import TranslationBackend
except ImportError:
    from base import TranslationBackend  # Colab ç›´æ¥ import
```

### 6.3 ç°¡é«”ä¸­æ–‡è¼¸å‡º

**ç—‡ç‹€**ï¼šç¿»è­¯çµæœå‡ºç¾ "è¿™äº›" è€Œé "é€™äº›"

**è§£æ±ºæ–¹æ³•**ï¼š
1. æª¢æŸ¥èªè¨€ä»£ç¢¼ï¼šä½¿ç”¨ `zh-TW` è€Œé `zho_Hant`
2. å®‰è£ hanziconvï¼š`!pip install hanziconv`
3. é©—è­‰å¾Œè™•ç†æœ‰åŸ·è¡Œ

### 6.4 è¨˜æ†¶é«”ä¸è¶³

```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**è§£æ±ºæ–¹æ³•**ï¼š

```python
# 1. é‡å•Ÿ runtime æ¸…ç©ºè¨˜æ†¶é«”
from IPython.display import clear_output
clear_output()

# 2. ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆæœªä¾†ç‰ˆæœ¬ï¼‰
# 3. æ¸›å°‘ batch size / max_tokens
# 4. ä½¿ç”¨ Colab Proï¼ˆæ›´å¤šè¨˜æ†¶é«”ï¼‰
```

---

## ä¸ƒã€éƒ¨ç½²é¸é …

é›–ç„¶æœ¬æ–‡èšç„¦ Colabï¼Œä½† TranslateGemma ä¹Ÿæ”¯æ´å…¶ä»–éƒ¨ç½²æ–¹å¼ï¼š

### 7.1 æœ¬åœ° macOS (Apple Silicon)

```bash
# ä½¿ç”¨ Ollamaï¼ˆæœ€ç°¡å–®ï¼‰
brew install ollama
ollama pull translategemma
ollama run translategemma "Translate to Traditional Chinese: Hello"

# æˆ–ä½¿ç”¨ MLXï¼ˆæœ€å¿«ï¼‰
pip install mlx-lm
mlx_lm.generate --model mlx-community/translategemma-4b-it \
  --prompt "Translate to Traditional Chinese: Hello"
```

**æ•ˆèƒ½**ï¼š
- Ollama: ~30 tok/s on M1
- MLX: ~230 tok/s on M1 (7-8x faster!)

### 7.2 Cloud Run GPU

```bash
# ä½¿ç”¨ TGI (Text Generation Inference)
gcloud beta run deploy translategemma \
  --image=us-docker.pkg.dev/.../huggingface-text-generation-inference-cu124 \
  --args="--model-id=google/translategemma-4b-it" \
  --gpu=1 \
  --gpu-type=nvidia-l4 \
  --region=us-central1
```

**æˆæœ¬ä¼°ç®—**ï¼š
- L4 GPU: ~$0.67/å°æ™‚
- Scale-to-zero: ç„¡è«‹æ±‚æ™‚ä¸è¨ˆè²»
- é©åˆï¼šAPI æœå‹™ã€ç”Ÿç”¢ç’°å¢ƒ

### 7.3 æœ¬åœ° Windows (NVIDIA GPU)

```bash
# ä½¿ç”¨ Ollama for Windows
ollama pull translategemma
ollama serve

# æˆ–ä½¿ç”¨ transformers
pip install transformers torch
python examples/translate.py --mode text --backend transformers
```

---

## å…«ã€æœªä¾†å±•æœ›

### 8.1 Gemma 3 ç³»åˆ—

Google å·²ç™¼å¸ƒ Gemma 3ï¼ˆ2025-12ï¼‰ï¼Œç›¸æ¯” TranslateGemma (Gemma 2 based) æœ‰ä»¥ä¸‹æ”¹é€²ï¼š

- ğŸš€ **æ›´å¿«æ¨ç†**ï¼š3x faster on same hardware
- ğŸ¯ **æ›´é«˜æº–ç¢ºåº¦**ï¼šBLEU score æå‡ 15%
- ğŸŒ **æ›´å¤šèªè¨€**ï¼šæ“´å±•åˆ° 100+ èªè¨€
- ğŸ“± **è¼•é‡åŒ–ç‰ˆæœ¬**ï¼š1B æ¨¡å‹å¯åœ¨æ‰‹æ©Ÿé‹è¡Œ

### 8.2 å¯èƒ½çš„æ”¹é€²æ–¹å‘

**ç•¶å‰é™åˆ¶**ï¼š
- âš ï¸ é•·æ–‡æœ¬ç¿»è­¯ï¼ˆ>1000 tokensï¼‰éœ€åˆ†æ®µ
- âš ï¸ å°ˆæ¥­è¡“èªç¿»è­¯æº–ç¢ºåº¦ä»éœ€æ”¹é€²
- âš ï¸ ç¼ºä¹é›™å‘å°ç…§åŠŸèƒ½

**æœªä¾†åŠŸèƒ½**ï¼š
- [ ] æ”¯æ´ Streaming è¼¸å‡ºï¼ˆé‚Šç¿»è­¯é‚Šé¡¯ç¤ºï¼‰
- [ ] æ•´åˆ RAGï¼ˆæª¢ç´¢å°ˆæ¥­è¡“èªåº«ï¼‰
- [ ] æ”¯æ´ batch APIï¼ˆåŒæ™‚ç¿»è­¯å¤šå€‹æ–‡ä»¶ï¼‰
- [ ] åŠ å…¥å“è³ªè©•ä¼°ï¼ˆBLEU/COMET åˆ†æ•¸ï¼‰
- [ ] å¾®èª¿ä»‹é¢ï¼ˆé‡å°ç‰¹å®šé ˜åŸŸè¨“ç·´ï¼‰

### 8.3 ç¤¾ç¾¤è²¢ç»

æ­¡è¿åˆ° GitHub repository æäº¤ PRï¼š

- ğŸ› Bug ä¿®å¾©
- âœ¨ æ–°åŠŸèƒ½å¯¦ä½œ
- ğŸ“š æ–‡ä»¶æ”¹é€²
- ğŸŒ æ›´å¤šèªè¨€æ”¯æ´

---

## ä¹ã€çµè«–

TranslateGemma ç‚ºé–‹æºç¿»è­¯å¸¶ä¾†äº†æ–°çš„å¯èƒ½æ€§ï¼š

âœ… **å…è²» GPU é‹ç®—**ï¼šColab T4 è¶³ä»¥é‹è¡Œ 4B æ¨¡å‹
âœ… **å¤šæ¨¡æ…‹èƒ½åŠ›**ï¼šåœ–æ–‡ä¸¦èŒ‚çš„å…§å®¹ä¹Ÿèƒ½æº–ç¢ºç¿»è­¯
âœ… **ç¹é«”ä¸­æ–‡æ”¯æ´**ï¼šé€éæ­£ç¢ºé…ç½®ç¢ºä¿è¼¸å‡ºç¹é«”
âœ… **å½ˆæ€§éƒ¨ç½²**ï¼šå¾ Colab åˆ°æœ¬åœ°åˆ°é›²ç«¯çš†å¯

ç„¡è«–ä½ æ˜¯æƒ³ï¼š
- ğŸ“– é–±è®€å¤–èªæŠ€è¡“æ–‡ä»¶
- ğŸ”¬ ç¿»è­¯ç ”ç©¶è«–æ–‡
- ğŸŒ ç€è¦½å¤–èªç¶²ç«™
- ğŸ± ç¿»è­¯èœå–®æˆ–æ¨™ç¤º

TranslateGemma éƒ½æä¾›äº†é–‹æºã€å…è²»ã€é«˜å“è³ªçš„è§£æ±ºæ–¹æ¡ˆã€‚

**ç«‹å³é–‹å§‹ä½ çš„ç¿»è­¯æ—…ç¨‹**ï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jimmyliao/trans-gemma/blob/main/document-translator-colab.ipynb)

---

## åƒè€ƒè³‡æº

- ğŸ“¦ [GitHub Repository](https://github.com/jimmyliao/trans-gemma)
- ğŸ¤— [TranslateGemma Model Card](https://huggingface.co/google/translategemma-4b-it)
- ğŸ“„ [TranslateGemma Technical Report (arXiv)](https://arxiv.org/abs/2601.09012)
- ğŸ“ [Google Blog: TranslateGemma](https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/)
- ğŸ“š [Examples Documentation](https://github.com/jimmyliao/trans-gemma/blob/main/examples/README.md)

---

**é—œæ–¼ä½œè€…**

Jimmy Liao - AI Google Developer Expert (GDE)ï¼ŒAI æ–°å‰µ CTO/å…±åŒå‰µè¾¦äººã€‚å°ˆæ³¨æ–¼æ™ºæ…§è£½é€ èˆ‡é‡‘èé ˜åŸŸï¼Œè‡´åŠ›æ–¼å°‡ AI æŠ€è¡“è½åœ°æ‡‰ç”¨ã€‚

- ğŸ¦ Twitter: [@jimmyliao](https://twitter.com/jimmyliao)
- ğŸ’¼ LinkedIn: [jimmyliao](https://linkedin.com/in/jimmyliao)
- ğŸ“ Blog: [memo.jimmyliao.net](https://memo.jimmyliao.net)

---

**æˆæ¬Šè²æ˜**

æœ¬æ–‡åŸºæ–¼ MIT License æˆæ¬Šã€‚ç¨‹å¼ç¢¼ç¯„ä¾‹å¯è‡ªç”±ç”¨æ–¼å•†æ¥­èˆ‡éå•†æ¥­ç”¨é€”ã€‚

**å…è²¬è²æ˜**

æœ¬æ–‡ç‚ºæ•™è‚²èˆ‡ç ”ç©¶ç›®çš„æä¾›ï¼Œä½œè€…èˆ‡ Google TranslateGemma åœ˜éšŠç„¡éš¸å±¬é—œä¿‚ã€‚ä½¿ç”¨æ™‚è«‹éµå®ˆç›¸é—œæˆæ¬Šæ¢æ¬¾ã€‚
