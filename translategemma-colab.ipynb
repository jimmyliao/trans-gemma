{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TranslateGemma 4B - Google Colab å¯¦é©—\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jimmyliao/trans-gemma/blob/main/translategemma-colab.ipynb)\n",
    "\n",
    "æœ¬ notebook å±•ç¤ºå¦‚ä½•åœ¨ Google Colab å…è²» GPU ä¸Šé‹è¡Œ TranslateGemma 4B ç¿»è­¯æ¨¡å‹ã€‚\n",
    "\n",
    "## ç’°å¢ƒéœ€æ±‚\n",
    "- Google Colab T4 GPUï¼ˆå…è²»ï¼‰\n",
    "- Python 3.10+\n",
    "- Transformers, PyTorch\n",
    "\n",
    "## ä½¿ç”¨æ–¹å¼\n",
    "1. é»æ“Šä¸Šæ–¹ã€ŒOpen In Colabã€æŒ‰éˆ•\n",
    "2. Runtime > Change runtime type > T4 GPU\n",
    "3. ä¾åºåŸ·è¡Œæ‰€æœ‰ cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæª¢æŸ¥èˆ‡è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ GPU æ˜¯å¦å¯ç”¨\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "!pip install -q transformers torch accelerate huggingface_hub sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ Python å’Œå¥—ä»¶ç‰ˆæœ¬\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1.5 Hugging Face èªè­‰ âš ï¸ é‡è¦ï¼\n\nTranslateGemma æ˜¯ä¸€å€‹ **gated repository**ï¼Œéœ€è¦å…ˆå®Œæˆä»¥ä¸‹æ­¥é©Ÿï¼š\n\n### æ­¥é©Ÿï¼š\n1. å‰å¾€ [Hugging Face TranslateGemma é é¢](https://huggingface.co/google/translategemma-4b-it)\n2. é»æ“Šã€ŒRequest accessã€ç”³è«‹å­˜å–æ¬Šé™ï¼ˆé€šå¸¸ç«‹å³æ‰¹å‡†ï¼‰\n3. å‰å¾€ [Token è¨­å®šé é¢](https://huggingface.co/settings/tokens) å»ºç«‹ access token\n4. åœ¨ Colab ä¸­ï¼Œé»æ“Šå·¦å´çš„ **ğŸ”‘ Secrets** åœ–ç¤º\n5. æ–°å¢ secret: `HF_TOKEN` = ä½ çš„ token\n6. å•Ÿç”¨ã€ŒNotebook accessã€\n\nè©³ç´°èªªæ˜è«‹åƒè€ƒï¼š[docs/huggingface-access.md](https://github.com/jimmyliao/trans-gemma/blob/main/docs/huggingface-access.md)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import login\n\ntry:\n    # æ–¹å¼ A: å¾ Colab Secrets è®€å–ï¼ˆæ¨è–¦ï¼‰\n    from google.colab import userdata\n    hf_token = userdata.get('HF_TOKEN')\n    login(token=hf_token)\n    print(\"âœ… æˆåŠŸä½¿ç”¨ Colab Secrets ç™»å…¥ Hugging Face\")\nexcept:\n    # æ–¹å¼ B: æ‰‹å‹•è¼¸å…¥ token\n    print(\"âš ï¸ æœªæ‰¾åˆ° HF_TOKEN secretï¼Œè«‹æ‰‹å‹•è¼¸å…¥ tokenï¼š\")\n    login()\n\nprint(\"ğŸ‰ Hugging Face èªè­‰å®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥ TranslateGemma 4B æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# æ¨¡å‹åç¨±\n",
    "MODEL_ID = \"google/translategemma-4b-it\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹ï¼ˆä½¿ç”¨ bfloat16 é™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  # è‡ªå‹•åˆ†é…åˆ° GPU\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŸºæœ¬ç¿»è­¯åŠŸèƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# èªè¨€ä»£ç¢¼æ˜ å°„ï¼ˆTranslateGemma ä½¿ç”¨ ISO 639-1 å…©ç¢¼æ¨™æº–ï¼‰\n# ä¿®æ­£é‡é»ï¼š\n# 1. æ”¹ç”¨ ISO 639-1ï¼ˆå…©ç¢¼ï¼‰ï¼Œä¾‹å¦‚ English æ˜¯ \"en\" è€Œé \"eng\"\n# 2. ä¸­æ–‡ä½¿ç”¨ \"zh-TW\" / \"zh-CN\" æ ¼å¼ï¼ˆç¬¦åˆæ¨¡å‹å° Regionalized variant çš„è¦æ±‚ï¼‰\n\nLANGUAGE_CODES = {\n    # --- ä¸»è¦å¸¸ç”¨èªè¨€ ---\n    \"English\": \"en\",\n    \"Traditional Chinese (Taiwan)\": \"zh-TW\",  # ç¹é«”ä¸­æ–‡\n    \"Simplified Chinese (China)\": \"zh-CN\",    # ç°¡é«”ä¸­æ–‡\n    \"Japanese\": \"ja\",\n    \"Korean\": \"ko\",\n    \n    # --- æ­æ´²èªè¨€ ---\n    \"French\": \"fr\",\n    \"German\": \"de\",\n    \"Spanish\": \"es\",\n    \"Italian\": \"it\",\n    \"Portuguese\": \"pt\",  # æˆ– \"pt-BR\" (å·´è¥¿), \"pt-PT\" (è‘¡è„ç‰™)\n    \"Russian\": \"ru\",\n    \"Ukrainian\": \"uk\",\n    \"Dutch\": \"nl\",\n    \"Polish\": \"pl\",\n    \"Czech\": \"cs\",\n    \"Turkish\": \"tr\",\n    \"Greek\": \"el\",\n    \"Bulgarian\": \"bg\",\n    \"Danish\": \"da\",\n    \"Finnish\": \"fi\",\n    \"Norwegian\": \"no\",\n    \"Swedish\": \"sv\",\n    \"Romanian\": \"ro\",\n    \"Hungarian\": \"hu\",\n    \n    # --- äºæ´²èˆ‡ä¸­æ±èªè¨€ ---\n    \"Arabic\": \"ar\",\n    \"Hindi\": \"hi\",\n    \"Vietnamese\": \"vi\",\n    \"Thai\": \"th\",\n    \"Indonesian\": \"id\",\n    \"Filipino\": \"fil\",  # æˆ– \"tl\"\n    \"Malay\": \"ms\",\n    \"Bengali\": \"bn\",\n    \"Hebrew\": \"he\",  # æœ‰æ™‚ä¹Ÿç”¨ \"iw\"\n    \"Persian\": \"fa\",\n}\n\ndef get_lang_code(lang_name):\n    \"\"\"\n    å–å¾—èªè¨€ä»£ç¢¼ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡æ‹‹å‡ºéŒ¯èª¤\n    \n    Args:\n        lang_name: èªè¨€åç¨±\n        \n    Returns:\n        å°æ‡‰çš„èªè¨€ä»£ç¢¼\n        \n    Raises:\n        ValueError: ç•¶èªè¨€ä¸è¢«æ”¯æ´æ™‚\n    \"\"\"\n    code = LANGUAGE_CODES.get(lang_name)\n    if not code:\n        supported_langs = \", \".join(list(LANGUAGE_CODES.keys())[:10]) + \"...\"\n        raise ValueError(\n            f\"Language '{lang_name}' not supported. \"\n            f\"Supported languages: {supported_langs}\"\n        )\n    return code\n\ndef translate(text, target_lang=\"Traditional Chinese (Taiwan)\", source_lang=\"English\", max_new_tokens=256):\n    \"\"\"\n    ç¿»è­¯æ–‡æœ¬åˆ°æŒ‡å®šèªè¨€ï¼ˆä½¿ç”¨ TranslateGemma æ­£ç¢ºæ ¼å¼ï¼‰\n\n    Args:\n        text: è¦ç¿»è­¯çš„æ–‡æœ¬\n        target_lang: ç›®æ¨™èªè¨€åç¨±\n        source_lang: ä¾†æºèªè¨€åç¨±ï¼ˆé è¨­ï¼šEnglishï¼‰\n        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•¸\n\n    Returns:\n        ç¿»è­¯çµæœ\n    \"\"\"\n    # å–å¾—èªè¨€ä»£ç¢¼ï¼ˆæœƒè‡ªå‹•é©—è­‰ï¼‰\n    source_code = get_lang_code(source_lang)\n    target_code = get_lang_code(target_lang)\n\n    # TranslateGemma çš„æ­£ç¢ºæ ¼å¼ âš ï¸ é‡è¦ï¼\n    # å¿…é ˆä½¿ç”¨åŒ…å« source_lang_code å’Œ target_lang_code çš„çµæ§‹\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": text,\n                    \"source_lang_code\": source_code,\n                    \"target_lang_code\": target_code\n                }\n            ]\n        }\n    ]\n\n    # ä½¿ç”¨ chat template\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        return_tensors=\"pt\",\n        add_generation_prompt=True\n    ).to(model.device)\n\n    # ç”Ÿæˆç¿»è­¯\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,  # ä½¿ç”¨ greedy decoding ç¢ºä¿ä¸€è‡´æ€§\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # è§£ç¢¼çµæœ\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # TranslateGemma é€šå¸¸ç›´æ¥è¿”å›ç¿»è­¯çµæœ\n    # å–æœ€å¾Œä¸€è¡Œä½œç‚ºç¿»è­¯æ–‡æœ¬\n    lines = result.strip().split('\\n')\n    translation = lines[-1].strip() if lines else result.strip()\n\n    return translation\n\n# æ¸¬è©¦ç¿»è­¯å‡½æ•¸\nprint(\"âœ… ç¿»è­¯å‡½æ•¸å·²è¼‰å…¥ï¼\")\nprint(f\"\\næ”¯æ´çš„èªè¨€ï¼ˆå…± {len(LANGUAGE_CODES)} ç¨®ï¼‰ï¼š\")\nfor i, (lang, code) in enumerate(LANGUAGE_CODES.items(), 1):\n    print(f\"{i:2d}. {lang:35s} â†’ {code}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ¸¬è©¦è‹±æ–‡åˆ°ç¹é«”ä¸­æ–‡\ntest_text = \"Hello, how are you today? I hope you're having a great day!\"\nprint(f\"Original (EN): {test_text}\")\nprint(f\"\\nTranslated (ZH-TW): {translate(test_text, 'Traditional Chinese (Taiwan)')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦è‹±æ–‡åˆ°æ—¥æ–‡\n",
    "test_text = \"Machine learning is transforming the world.\"\n",
    "print(f\"Original (EN): {test_text}\")\n",
    "print(f\"\\nTranslated (JA): {translate(test_text, 'Japanese')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ¸¬è©¦ä¸­æ–‡åˆ°è‹±æ–‡\ntest_text = \"æˆ‘å–œæ­¡åœ¨é€±æœ«é–±è®€å’Œå¯«ç¨‹å¼ã€‚\"\nprint(f\"Original (ZH): {test_text}\")\nprint(f\"\\nTranslated (EN): {translate(test_text, 'English', 'Traditional Chinese (Taiwan)')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•ˆèƒ½è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\ndef benchmark_translation(text, target_lang=\"Traditional Chinese (Taiwan)\", num_runs=5):\n    \"\"\"\n    è©•ä¼°ç¿»è­¯æ•ˆèƒ½\n    \"\"\"\n    times = []\n    \n    for i in range(num_runs):\n        start = time.time()\n        result = translate(text, target_lang)\n        elapsed = time.time() - start\n        times.append(elapsed)\n        print(f\"Run {i+1}: {elapsed:.2f}s\")\n    \n    avg_time = sum(times) / len(times)\n    print(f\"\\nAverage time: {avg_time:.2f}s\")\n    print(f\"Min time: {min(times):.2f}s\")\n    print(f\"Max time: {max(times):.2f}s\")\n    \n    return result\n\n# åŸ·è¡ŒåŸºæº–æ¸¬è©¦\ntest_text = \"Artificial intelligence is changing how we live and work.\"\nprint(f\"Testing with: {test_text}\\n\")\nresult = benchmark_translation(test_text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ‰¹é‡ç¿»è­¯æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æº–å‚™å¤šå€‹æ¸¬è©¦å¥å­\ntest_sentences = [\n    \"Good morning!\",\n    \"Thank you for your help.\",\n    \"Where is the nearest restaurant?\",\n    \"I would like to book a hotel room.\",\n    \"The weather is beautiful today.\"\n]\n\nprint(\"Batch Translation Test (EN â†’ ZH-TW)\\n\")\nprint(\"=\" * 80)\n\nfor i, sentence in enumerate(test_sentences, 1):\n    translation = translate(sentence, \"Traditional Chinese (Taiwan)\")\n    print(f\"{i}. EN: {sentence}\")\n    print(f\"   ZH: {translation}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. API è¨­è¨ˆåŸå‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TranslationRequest:\n",
    "    \"\"\"ç¿»è­¯è«‹æ±‚\"\"\"\n",
    "    text: str\n",
    "    target_lang: str = \"Traditional Chinese\"\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@dataclass\n",
    "class TranslationResponse:\n",
    "    \"\"\"ç¿»è­¯å›æ‡‰\"\"\"\n",
    "    original: str\n",
    "    translated: str\n",
    "    target_lang: str\n",
    "    status: str = \"success\"\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class TranslationService:\n",
    "    \"\"\"ç¿»è­¯æœå‹™é¡åˆ¥ï¼ˆç”¨æ–¼ FastAPIï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def translate(self, request: TranslationRequest) -> TranslationResponse:\n",
    "        \"\"\"åŸ·è¡Œç¿»è­¯\"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ä¹‹å‰å®šç¾©çš„ translate å‡½æ•¸\n",
    "            translated = translate(\n",
    "                request.text,\n",
    "                request.target_lang,\n",
    "                request.max_tokens\n",
    "            )\n",
    "            \n",
    "            return TranslationResponse(\n",
    "                original=request.text,\n",
    "                translated=translated,\n",
    "                target_lang=request.target_lang,\n",
    "                status=\"success\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return TranslationResponse(\n",
    "                original=request.text,\n",
    "                translated=\"\",\n",
    "                target_lang=request.target_lang,\n",
    "                status=\"error\",\n",
    "                error=str(e)\n",
    "            )\n",
    "\n",
    "# æ¸¬è©¦æœå‹™\n",
    "service = TranslationService(model, tokenizer)\n",
    "request = TranslationRequest(\n",
    "    text=\"Hello, world!\",\n",
    "    target_lang=\"Traditional Chinese\"\n",
    ")\n",
    "response = service.translate(request)\n",
    "\n",
    "print(f\"Status: {response.status}\")\n",
    "print(f\"Original: {response.original}\")\n",
    "print(f\"Translated: {response.translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. äº’å‹•å¼ç¿»è­¯å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ”¯æ´çš„èªè¨€åˆ—è¡¨ï¼ˆä½¿ç”¨å­—å…¸çš„ keysï¼‰\nSUPPORTED_LANGUAGES = list(LANGUAGE_CODES.keys())\n\ndef interactive_translate():\n    \"\"\"äº’å‹•å¼ç¿»è­¯å·¥å…·\"\"\"\n    print(\"=\" * 80)\n    print(\"TranslateGemma Interactive Tool\")\n    print(\"=\" * 80)\n    print(f\"\\nSupported Languages ({len(SUPPORTED_LANGUAGES)} total):\")\n    for i, lang in enumerate(SUPPORTED_LANGUAGES, 1):\n        print(f\"{i:2d}. {lang}\")\n    print(\"\\nType 'quit' to exit\\n\")\n    \n    while True:\n        text = input(\"\\nEnter text to translate (or 'quit'): \").strip()\n        if text.lower() == 'quit':\n            break\n        \n        if not text:\n            print(\"Please enter some text.\")\n            continue\n        \n        # è‡ªå‹•åµæ¸¬ä¾†æºèªè¨€ï¼ˆç°¡å–®åˆ¤æ–·ï¼‰\n        source_lang = \"English\"  # é è¨­è‹±æ–‡\n        if any('\\u4e00' <= char <= '\\u9fff' for char in text):\n            # åŒ…å«ä¸­æ–‡å­—å…ƒ\n            source_lang = \"Traditional Chinese (Taiwan)\"  # é è¨­ç¹é«”\n        \n        print(f\"Auto-detected source language: {source_lang}\")\n        \n        lang_input = input(\"Target language (name or number): \").strip()\n        \n        # è™•ç†æ•¸å­—è¼¸å…¥\n        if lang_input.isdigit():\n            lang_idx = int(lang_input) - 1\n            if 0 <= lang_idx < len(SUPPORTED_LANGUAGES):\n                target_lang = SUPPORTED_LANGUAGES[lang_idx]\n            else:\n                print(\"Invalid language number.\")\n                continue\n        else:\n            target_lang = lang_input\n        \n        try:\n            print(f\"\\nTranslating from {source_lang} to {target_lang}...\")\n            result = translate(text, target_lang, source_lang)\n            print(f\"Result: {result}\")\n        except ValueError as e:\n            print(f\"Error: {e}\")\n            print(\"Please use the language name exactly as shown in the list above.\")\n\n# åŸ·è¡Œäº’å‹•å¼å·¥å…·ï¼ˆåœ¨ Colab ä¸­å¯ç”¨ï¼‰\n# interactive_translate()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. éƒ¨ç½²æº–å‚™\n",
    "\n",
    "ä»¥ä¸‹ä»£ç¢¼æº–å‚™éƒ¨ç½²åˆ° Cloud Run æ‰€éœ€çš„æª”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ requirements.txt\n",
    "requirements = \"\"\"transformers>=4.36.0\n",
    "torch>=2.1.0\n",
    "accelerate>=0.25.0\n",
    "huggingface_hub>=0.20.0\n",
    "sentencepiece>=0.1.99\n",
    "protobuf>=4.25.0\n",
    "fastapi>=0.109.0\n",
    "uvicorn[standard]>=0.27.0\n",
    "pydantic>=2.5.0\n",
    "\"\"\"\n",
    "\n",
    "print(\"requirements.txt content:\")\n",
    "print(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI æ‡‰ç”¨ç¯„ä¾‹ï¼ˆmain.pyï¼‰\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "app = FastAPI(title=\"TranslateGemma API\")\n",
    "\n",
    "# å…¨åŸŸè®Šæ•¸\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "class TranslationRequest(BaseModel):\n",
    "    text: str\n",
    "    target_lang: str = \"Traditional Chinese\"\n",
    "    max_tokens: int = 256\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    original: str\n",
    "    translated: str\n",
    "    target_lang: str\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global model, tokenizer\n",
    "    MODEL_ID = \"google/translategemma-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"TranslateGemma API\", \"version\": \"1.0.0\"}\n",
    "\n",
    "@app.post(\"/translate\", response_model=TranslationResponse)\n",
    "async def translate(request: TranslationRequest):\n",
    "    try:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Translate this to {request.target_lang}: {request.text}\"\n",
    "        }]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"Translate this to\" in result:\n",
    "            result = result.split(\"\\\\n\")[-1].strip()\n",
    "        \n",
    "        return TranslationResponse(\n",
    "            original=request.text,\n",
    "            translated=result,\n",
    "            target_lang=request.target_lang\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "'''\n",
    "\n",
    "print(\"FastAPI main.py content:\")\n",
    "print(fastapi_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ç¸½çµèˆ‡ä¸‹ä¸€æ­¥\n",
    "\n",
    "æœ¬ notebook å®Œæˆäº†ä»¥ä¸‹å…§å®¹ï¼š\n",
    "\n",
    "âœ… åœ¨ Google Colab T4 GPU ä¸ŠæˆåŠŸè¼‰å…¥ TranslateGemma 4B\n",
    "âœ… å¯¦ä½œåŸºæœ¬ç¿»è­¯åŠŸèƒ½\n",
    "âœ… æ•ˆèƒ½è©•ä¼°èˆ‡åŸºæº–æ¸¬è©¦\n",
    "âœ… æ‰¹é‡ç¿»è­¯æ¸¬è©¦\n",
    "âœ… API è¨­è¨ˆåŸå‹\n",
    "âœ… éƒ¨ç½²æº–å‚™ï¼ˆrequirements.txt, main.pyï¼‰\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. **éƒ¨ç½²åˆ° Cloud Run**:\n",
    "   - ä½¿ç”¨ `cloudrun/deploy.sh` è…³æœ¬\n",
    "   - æˆ–é€é GitHub Actions è‡ªå‹•éƒ¨ç½²\n",
    "\n",
    "2. **æ•ˆèƒ½å„ªåŒ–**:\n",
    "   - å¯¦ä½œæ‰¹æ¬¡è™•ç†\n",
    "   - ä½¿ç”¨ vLLM æˆ– TGI åŠ é€Ÿæ¨ç†\n",
    "   - æ¨¡å‹é‡åŒ–ï¼ˆINT8/INT4ï¼‰\n",
    "\n",
    "3. **åŠŸèƒ½æ“´å±•**:\n",
    "   - æ”¯æ´æ›´å¤šèªè¨€å°\n",
    "   - å¯¦ä½œç¿»è­¯å“è³ªè©•ä¼°\n",
    "   - åŠ å…¥å¿«å–æ©Ÿåˆ¶\n",
    "\n",
    "### ç›¸é—œè³‡æº\n",
    "\n",
    "- [TranslateGemma å®˜æ–¹æ–‡æª”](https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/)\n",
    "- [Cloud Run GPU éƒ¨ç½²æŒ‡å—](https://cloud.google.com/run/docs/configuring/services/gpu)\n",
    "- [å°ˆæ¡ˆ GitHub Repository](https://github.com/jimmyliao/trans-gemma)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}