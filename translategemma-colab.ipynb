{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TranslateGemma 4B - Google Colab 實驗\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jimmyliao/trans-gemma/blob/main/translategemma-colab.ipynb)\n",
    "\n",
    "本 notebook 展示如何在 Google Colab 免費 GPU 上運行 TranslateGemma 4B 翻譯模型。\n",
    "\n",
    "## 環境需求\n",
    "- Google Colab T4 GPU（免費）\n",
    "- Python 3.10+\n",
    "- Transformers, PyTorch\n",
    "\n",
    "## 使用方式\n",
    "1. 點擊上方「Open In Colab」按鈕\n",
    "2. Runtime > Change runtime type > T4 GPU\n",
    "3. 依序執行所有 cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境檢查與設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 GPU 是否可用\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "!pip install -q transformers torch accelerate huggingface_hub sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 Python 和套件版本\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 載入 TranslateGemma 4B 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 模型名稱\n",
    "MODEL_ID = \"google/translategemma-4b-it\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 載入模型（使用 bfloat16 降低記憶體使用）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  # 自動分配到 GPU\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 基本翻譯功能測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, target_lang=\"Traditional Chinese\", max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    翻譯文本到指定語言\n",
    "    \n",
    "    Args:\n",
    "        text: 要翻譯的文本\n",
    "        target_lang: 目標語言（例如：Traditional Chinese, Japanese, French）\n",
    "        max_new_tokens: 最大生成 token 數\n",
    "    \n",
    "    Returns:\n",
    "        翻譯結果\n",
    "    \"\"\"\n",
    "    # 構建 prompt\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Translate this to {target_lang}: {text}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 使用 chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 生成翻譯\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # 使用 greedy decoding 確保一致性\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解碼結果\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 提取翻譯部分（移除 prompt）\n",
    "    # TranslateGemma 的輸出格式通常包含完整對話，我們只需要翻譯結果\n",
    "    if \"Translate this to\" in result:\n",
    "        result = result.split(\"\\n\")[-1].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試英文到繁體中文\n",
    "test_text = \"Hello, how are you today? I hope you're having a great day!\"\n",
    "print(f\"Original (EN): {test_text}\")\n",
    "print(f\"\\nTranslated (ZH-TW): {translate(test_text, 'Traditional Chinese')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試英文到日文\n",
    "test_text = \"Machine learning is transforming the world.\"\n",
    "print(f\"Original (EN): {test_text}\")\n",
    "print(f\"\\nTranslated (JA): {translate(test_text, 'Japanese')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試中文到英文\n",
    "test_text = \"我喜歡在週末閱讀和寫程式。\"\n",
    "print(f\"Original (ZH): {test_text}\")\n",
    "print(f\"\\nTranslated (EN): {translate(test_text, 'English')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 效能評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_translation(text, target_lang=\"Traditional Chinese\", num_runs=5):\n",
    "    \"\"\"\n",
    "    評估翻譯效能\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start = time.time()\n",
    "        result = translate(text, target_lang)\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        print(f\"Run {i+1}: {elapsed:.2f}s\")\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"\\nAverage time: {avg_time:.2f}s\")\n",
    "    print(f\"Min time: {min(times):.2f}s\")\n",
    "    print(f\"Max time: {max(times):.2f}s\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 執行基準測試\n",
    "test_text = \"Artificial intelligence is changing how we live and work.\"\n",
    "print(f\"Testing with: {test_text}\\n\")\n",
    "result = benchmark_translation(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 批量翻譯測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備多個測試句子\n",
    "test_sentences = [\n",
    "    \"Good morning!\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"Where is the nearest restaurant?\",\n",
    "    \"I would like to book a hotel room.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "print(\"Batch Translation Test (EN → ZH-TW)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    translation = translate(sentence, \"Traditional Chinese\")\n",
    "    print(f\"{i}. EN: {sentence}\")\n",
    "    print(f\"   ZH: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. API 設計原型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TranslationRequest:\n",
    "    \"\"\"翻譯請求\"\"\"\n",
    "    text: str\n",
    "    target_lang: str = \"Traditional Chinese\"\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@dataclass\n",
    "class TranslationResponse:\n",
    "    \"\"\"翻譯回應\"\"\"\n",
    "    original: str\n",
    "    translated: str\n",
    "    target_lang: str\n",
    "    status: str = \"success\"\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class TranslationService:\n",
    "    \"\"\"翻譯服務類別（用於 FastAPI）\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def translate(self, request: TranslationRequest) -> TranslationResponse:\n",
    "        \"\"\"執行翻譯\"\"\"\n",
    "        try:\n",
    "            # 使用之前定義的 translate 函數\n",
    "            translated = translate(\n",
    "                request.text,\n",
    "                request.target_lang,\n",
    "                request.max_tokens\n",
    "            )\n",
    "            \n",
    "            return TranslationResponse(\n",
    "                original=request.text,\n",
    "                translated=translated,\n",
    "                target_lang=request.target_lang,\n",
    "                status=\"success\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return TranslationResponse(\n",
    "                original=request.text,\n",
    "                translated=\"\",\n",
    "                target_lang=request.target_lang,\n",
    "                status=\"error\",\n",
    "                error=str(e)\n",
    "            )\n",
    "\n",
    "# 測試服務\n",
    "service = TranslationService(model, tokenizer)\n",
    "request = TranslationRequest(\n",
    "    text=\"Hello, world!\",\n",
    "    target_lang=\"Traditional Chinese\"\n",
    ")\n",
    "response = service.translate(request)\n",
    "\n",
    "print(f\"Status: {response.status}\")\n",
    "print(f\"Original: {response.original}\")\n",
    "print(f\"Translated: {response.translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 記憶體使用分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 互動式翻譯工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支援的語言列表\n",
    "SUPPORTED_LANGUAGES = [\n",
    "    \"Traditional Chinese\",\n",
    "    \"Simplified Chinese\",\n",
    "    \"Japanese\",\n",
    "    \"Korean\",\n",
    "    \"French\",\n",
    "    \"German\",\n",
    "    \"Spanish\",\n",
    "    \"Italian\",\n",
    "    \"Portuguese\",\n",
    "    \"Russian\",\n",
    "    \"Arabic\",\n",
    "    \"Hindi\",\n",
    "    \"Vietnamese\",\n",
    "    \"Thai\",\n",
    "    \"Indonesian\"\n",
    "]\n",
    "\n",
    "def interactive_translate():\n",
    "    \"\"\"互動式翻譯工具\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TranslateGemma Interactive Tool\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nSupported Languages:\")\n",
    "    for i, lang in enumerate(SUPPORTED_LANGUAGES, 1):\n",
    "        print(f\"{i:2d}. {lang}\")\n",
    "    print(\"\\nType 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nEnter text to translate (or 'quit'): \").strip()\n",
    "        if text.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not text:\n",
    "            print(\"Please enter some text.\")\n",
    "            continue\n",
    "        \n",
    "        lang_input = input(\"Target language (name or number): \").strip()\n",
    "        \n",
    "        # 處理數字輸入\n",
    "        if lang_input.isdigit():\n",
    "            lang_idx = int(lang_input) - 1\n",
    "            if 0 <= lang_idx < len(SUPPORTED_LANGUAGES):\n",
    "                target_lang = SUPPORTED_LANGUAGES[lang_idx]\n",
    "            else:\n",
    "                print(\"Invalid language number.\")\n",
    "                continue\n",
    "        else:\n",
    "            target_lang = lang_input\n",
    "        \n",
    "        print(f\"\\nTranslating to {target_lang}...\")\n",
    "        result = translate(text, target_lang)\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "# 執行互動式工具（在 Colab 中可用）\n",
    "# interactive_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 部署準備\n",
    "\n",
    "以下代碼準備部署到 Cloud Run 所需的檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 requirements.txt\n",
    "requirements = \"\"\"transformers>=4.36.0\n",
    "torch>=2.1.0\n",
    "accelerate>=0.25.0\n",
    "huggingface_hub>=0.20.0\n",
    "sentencepiece>=0.1.99\n",
    "protobuf>=4.25.0\n",
    "fastapi>=0.109.0\n",
    "uvicorn[standard]>=0.27.0\n",
    "pydantic>=2.5.0\n",
    "\"\"\"\n",
    "\n",
    "print(\"requirements.txt content:\")\n",
    "print(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI 應用範例（main.py）\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "app = FastAPI(title=\"TranslateGemma API\")\n",
    "\n",
    "# 全域變數\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "class TranslationRequest(BaseModel):\n",
    "    text: str\n",
    "    target_lang: str = \"Traditional Chinese\"\n",
    "    max_tokens: int = 256\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    original: str\n",
    "    translated: str\n",
    "    target_lang: str\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global model, tokenizer\n",
    "    MODEL_ID = \"google/translategemma-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"TranslateGemma API\", \"version\": \"1.0.0\"}\n",
    "\n",
    "@app.post(\"/translate\", response_model=TranslationResponse)\n",
    "async def translate(request: TranslationRequest):\n",
    "    try:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Translate this to {request.target_lang}: {request.text}\"\n",
    "        }]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"Translate this to\" in result:\n",
    "            result = result.split(\"\\\\n\")[-1].strip()\n",
    "        \n",
    "        return TranslationResponse(\n",
    "            original=request.text,\n",
    "            translated=result,\n",
    "            target_lang=request.target_lang\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "'''\n",
    "\n",
    "print(\"FastAPI main.py content:\")\n",
    "print(fastapi_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結與下一步\n",
    "\n",
    "本 notebook 完成了以下內容：\n",
    "\n",
    "✅ 在 Google Colab T4 GPU 上成功載入 TranslateGemma 4B\n",
    "✅ 實作基本翻譯功能\n",
    "✅ 效能評估與基準測試\n",
    "✅ 批量翻譯測試\n",
    "✅ API 設計原型\n",
    "✅ 部署準備（requirements.txt, main.py）\n",
    "\n",
    "### 下一步\n",
    "\n",
    "1. **部署到 Cloud Run**:\n",
    "   - 使用 `cloudrun/deploy.sh` 腳本\n",
    "   - 或透過 GitHub Actions 自動部署\n",
    "\n",
    "2. **效能優化**:\n",
    "   - 實作批次處理\n",
    "   - 使用 vLLM 或 TGI 加速推理\n",
    "   - 模型量化（INT8/INT4）\n",
    "\n",
    "3. **功能擴展**:\n",
    "   - 支援更多語言對\n",
    "   - 實作翻譯品質評估\n",
    "   - 加入快取機制\n",
    "\n",
    "### 相關資源\n",
    "\n",
    "- [TranslateGemma 官方文檔](https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/)\n",
    "- [Cloud Run GPU 部署指南](https://cloud.google.com/run/docs/configuring/services/gpu)\n",
    "- [專案 GitHub Repository](https://github.com/jimmyliao/trans-gemma)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
