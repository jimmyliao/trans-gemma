{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TranslateGemma - Document Translator\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jimmyliao/trans-gemma/blob/main/document-translator-colab.ipynb)\n\nTranslate PDFs, images, and websites to Traditional Chinese (zh-TW) using Google's TranslateGemma model.\n\n**Features:**\n- \ud83d\udcc4 Download from arXiv automatically\n- \ud83d\uddbc\ufe0f Upload images or PDFs\n- \ud83c\udf10 Screenshot websites and translate\n- \ud83d\ude80 Fast GPU inference on Colab (T4)\n- \ud83c\uddf9\ud83c\uddfc Force Traditional Chinese output (configurable)\n\n**Single Source of Truth:** Uses the same code from [trans-gemma repo](https://github.com/jimmyliao/trans-gemma)\n\n---\n\n## \ud83d\udc64 About the Author\n\n**Jimmy Liao** - AI GDE (Google Developer Expert), CTO/Co-Founder of AI Startup\n\nDedicated to smart manufacturing and finance sectors, focusing on transforming technical challenges from AI advancement into competitive advantages while enhancing client value and operational efficiency.\n\n- \ud83d\udc26 Twitter: [@jimmyliao](https://twitter.com/jimmyliao)\n- \ud83d\udcbc LinkedIn: [jimmyliao](https://linkedin.com/in/jimmyliao)\n- \ud83d\udcdd Blog: [memo.jimmyliao.net](https://memo.jimmyliao.net)\n- \ud83d\udd17 Sessionize: [jimmy-liao](https://sessionize.com/jimmy-liao/)\n\n---\n\n**Disclaimer:** This notebook is provided for educational and research purposes. The author is not affiliated with Google's TranslateGemma team. Use at your own discretion."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Setup: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean up existing directory if it exists\n!rm -rf trans-gemma\n\n# Clone the repository (single source of truth)\n!git clone https://github.com/jimmyliao/trans-gemma.git\n%cd trans-gemma"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!pip install uv -q\n",
    "\n",
    "# Install project dependencies\n",
    "!uv pip install --system -e \".[examples]\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5\ufe0f\u20e3 HuggingFace Authentication\n\n**IMPORTANT:** TranslateGemma is a gated model. You need to:\n1. Get a HuggingFace token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n2. Accept model access at [https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)\n\n### \ud83d\udd10 Configuration Methods (Choose ONE based on your environment)\n\n#### **Option A: Web Colab** (Using browser)\n1. Click the \ud83d\udd11 icon on left sidebar\n2. Add secret: `HF_TOKEN` = your token\n3. Run the cell below \u2192 Token loaded automatically from Colab Secrets\n\n#### **Option B: VS Code Colab Extension** (Using VS Code locally)\n\n**\u26a0\ufe0f Important:** Your local `.env` file is NOT automatically synced to remote Colab runtime!\n\n**Solution: Create .env in remote runtime**\n\nThe cell below will:\n1. First check if `.env` exists in remote runtime\n2. If not found, prompt you to enter token\n3. Automatically create `.env` file in remote runtime\n4. Use this token for authentication\n\nThis way, you only need to enter your token once per runtime session.\n\n#### **Option C: Manual Input Every Time** (Not recommended)\nSkip .env creation and enter token manually each time.\n\n---\n\n**What happens when you run the cell below?**\n1. Checks for `.env` in current directory (remote runtime)\n2. If not found, prompts for token and creates `.env`\n3. If found, reads token from `.env`\n4. Authenticates with HuggingFace\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import login\nimport os\nfrom pathlib import Path\n\ndef get_hf_token():\n    \"\"\"Smart HF Token retrieval with .env creation for VS Code\"\"\"\n    \n    # Method 1: Try .env file in current directory\n    env_file = Path('.env')\n    \n    if env_file.exists():\n        try:\n            with open('.env', 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('HF_TOKEN='):\n                        token = line.split('=', 1)[1].strip().strip('\"').strip(\"'\")\n                        if token:\n                            print(\"\u2705 HF_TOKEN loaded from .env file\")\n                            return token\n            print(\"\u26a0\ufe0f  .env file found but HF_TOKEN not set correctly\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Error reading .env: {e}\")\n    \n    # Method 2: Try environment variables\n    token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n    if token:\n        print(\"\u2705 HF_TOKEN loaded from environment variable\")\n        return token\n    \n    # Method 3: Try Colab Secrets (Web Colab only)\n    try:\n        from google.colab import userdata\n        token = userdata.get('HF_TOKEN')\n        print(\"\u2705 HF_TOKEN loaded from Colab Secrets (Web Colab)\")\n        return token\n    except Exception:\n        pass\n    \n    # Method 4: Prompt for token and create .env (VS Code friendly)\n    print(\"\\n\" + \"=\"*80)\n    print(\"\u26a0\ufe0f  HF_TOKEN not found - Creating .env file\")\n    print(\"=\"*80)\n    print(\"\\n\ud83d\udcdd Please enter your HuggingFace token:\")\n    print(\"   Get token: https://huggingface.co/settings/tokens\")\n    print(\"   Accept access: https://huggingface.co/google/translategemma-4b-it\")\n    print(\"\\n\ud83d\udca1 Your token will be saved to .env for this runtime session\")\n    print()\n    \n    token = input(\"HuggingFace Token: \").strip()\n    \n    if token:\n        # Save to .env for future use in this session\n        try:\n            with open('.env', 'w') as f:\n                f.write(f\"HF_TOKEN={token}\\n\")\n            print(\"\\n\u2705 Token saved to .env file (runtime session)\")\n            print(\"   Next time you run this cell, it will load automatically\")\n        except Exception as e:\n            print(f\"\\n\u26a0\ufe0f  Could not save to .env: {e}\")\n            print(\"   Token will work this time but won't persist\")\n        \n        return token\n    else:\n        raise ValueError(\"\u274c HF_TOKEN is required to use TranslateGemma\")\n\n# Authenticate\ntry:\n    HF_TOKEN = get_hf_token()\n    os.environ['HF_TOKEN'] = HF_TOKEN\n    login(token=HF_TOKEN)\n    print(\"\\n\u2705 Successfully authenticated with HuggingFace\\n\")\nexcept Exception as e:\n    print(f\"\\n\u274c Authentication failed: {e}\\n\")\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3\ufe0f\u20e3 Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Target language (default: Traditional Chinese)\nTARGET_LANG = \"zh-TW\"  # Change to \"zh-CN\", \"ja\", \"ko\", etc. if needed\n\n# Backend (transformers is best for Colab GPU)\nBACKEND = \"transformers\"\n\nprint(f\"\u2705 Target language: {TARGET_LANG}\")\nprint(f\"\u2705 Backend: {BACKEND}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Option A: Download from arXiv\n",
    "\n",
    "Automatically download and translate arXiv papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter arXiv ID (e.g., \"2601.09012v2\" or \"2601.09012\")\n",
    "ARXIV_ID = \"2601.09012v2\"  # TranslateGemma technical report\n",
    "\n",
    "# Translate specific pages (1-indexed)\n",
    "START_PAGE = 1\n",
    "END_PAGE = 1  # Set to None for all pages\n",
    "\n",
    "# Build command\n",
    "cmd = f\"python examples/translate.py --mode pdf --arxiv {ARXIV_ID} --backend {BACKEND} --target {TARGET_LANG}\"\n",
    "if START_PAGE:\n",
    "    cmd += f\" --start-page {START_PAGE}\"\n",
    "if END_PAGE:\n",
    "    cmd += f\" --end-page {END_PAGE}\"\n",
    "\n",
    "print(f\"Running: {cmd}\\n\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Option B: Upload PDF\n",
    "\n",
    "Upload your own PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nimport os\n\n# Upload PDF\nprint(\"\ud83d\udce4 Please upload your PDF file:\")\nuploaded = files.upload()\n\n# Get uploaded filename\npdf_file = list(uploaded.keys())[0]\nprint(f\"\\n\u2705 Uploaded: {pdf_file}\")\n\n# Translate settings\nSTART_PAGE = 1\nEND_PAGE = 3  # Change as needed\nUSE_IMAGE_MODE = False  # Set to True for multimodal (slower but preserves layout)\nDPI = 96  # For image mode: lower = faster (72, 96, or 150)\n\n# Build command\ncmd = f\"python examples/translate.py --mode pdf --file {pdf_file} --backend {BACKEND} --target {TARGET_LANG}\"\nif START_PAGE:\n    cmd += f\" --start-page {START_PAGE}\"\nif END_PAGE:\n    cmd += f\" --end-page {END_PAGE}\"\nif USE_IMAGE_MODE:\n    cmd += f\" --pdf-as-image --dpi {DPI}\"\n\nprint(f\"\\nRunning: {cmd}\\n\")\n!{cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Option C: PDF with Image Mode (Multimodal)\n",
    "\n",
    "Use multimodal TranslateGemma to preserve visual context (tables, charts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Translate PDF page with charts/figures (multimodal)\nARXIV_ID = \"2601.09012v2\"\nSTART_PAGE = 3  # Page with Figure 1 (language distribution charts)\nEND_PAGE = 3\nDPI = 96  # Lower DPI = faster (72, 96, or 150)\n\ncmd = f\"python examples/translate.py --mode pdf --arxiv {ARXIV_ID} --backend {BACKEND} --target {TARGET_LANG} --pdf-as-image --dpi {DPI}\"\nif START_PAGE:\n    cmd += f\" --start-page {START_PAGE}\"\nif END_PAGE:\n    cmd += f\" --end-page {END_PAGE}\"\n\nprint(f\"Running: {cmd}\\n\")\n!{cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5\ufe0f\u20e3 Single Image Translation\n\nTranslate text from a single image using multimodal TranslateGemma."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nfrom PIL import Image\nimport sys\nimport urllib.request\nimport os\n\n# Add examples directory to path\nsys.path.insert(0, 'examples')\nsys.path.insert(0, 'examples/backends')\n\n# Configuration: Choose image source\nUSE_DEFAULT_IMAGE = True  # Set to False to upload your own image\nDEFAULT_IMAGE_URL = \"https://cdn.odigo.net/f91b9c108a1e0cd1117e1c46ee36eeca.jpg\"\n\n# Language configuration\nSOURCE_LANG = \"ja\"  # This is a Japanese menu image\n\n# Get image\nif USE_DEFAULT_IMAGE:\n    print(f\"\ud83d\udce5 Downloading default image from:\\n   {DEFAULT_IMAGE_URL}\\n\")\n    image_file = \"demo_image.jpg\"\n    urllib.request.urlretrieve(DEFAULT_IMAGE_URL, image_file)\n    print(f\"\u2705 Downloaded: {image_file}\")\nelse:\n    print(\"\ud83d\udce4 Please upload your image:\")\n    uploaded = files.upload()\n    image_file = list(uploaded.keys())[0]\n    print(f\"\\n\u2705 Uploaded: {image_file}\")\n\n# Load backend\nfrom transformers_multimodal_backend import TransformersMultimodalBackend\n\nprint(\"\\n\ud83d\udd04 Loading multimodal backend...\")\nbackend = TransformersMultimodalBackend()\nbackend.load_model()\n\n# Translate\nprint(f\"\\n\ud83d\udd04 Translating {image_file}...\")\nprint(f\"Source language: {SOURCE_LANG} \u2192 Target language: {TARGET_LANG}\")\nresult = backend.translate_image(image_file, source_lang=SOURCE_LANG, target_lang=TARGET_LANG)\n\n# Display result\nprint(f\"\\n\u2705 Translation:\")\nprint(result['translation'])\nprint(f\"\\n\u23f1\ufe0f  Time: {result['time']:.2f}s, Speed: {result['metadata']['tokens_per_second']:.1f} tok/s\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 6\ufe0f\u20e3 Website Article Translation (Web Scraping)\n\nExtract text from websites and translate them accurately using web scraping instead of screenshots.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install web scraping dependencies\n!pip install beautifulsoup4 requests -q\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport sys\nimport time\nsys.path.insert(0, 'examples')\nsys.path.insert(0, 'examples/backends')\n\n# Configuration\nARTICLE_URL = \"https://aismiley.co.jp/ai_news/gemma3-rag-api-local-use/\"\nSOURCE_LANG = \"ja\"  # Japanese article\n\ndef extract_article_text(url):\n    \"\"\"Extract main article content from webpage\"\"\"\n    print(f\"\ud83c\udf10 Fetching webpage: {url}\\n\")\n    \n    # Fetch webpage\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    response = requests.get(url, headers=headers)\n    response.encoding = response.apparent_encoding\n    \n    # Parse HTML\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Extract title from h1\n    title = soup.find('h1')\n    title_text = title.get_text(strip=True) if title else \"No title found\"\n    \n    # Remove unwanted elements (navigation, sidebar, footer, scripts)\n    for element in soup.select('nav, aside, footer, script, style, .sidebar, .navigation, .menu, .footer, .header'):\n        element.decompose()\n    \n    # Try to find main content area with multiple strategies\n    content_area = None\n    \n    # Strategy 1: Look for specific content containers\n    content_selectors = [\n        'main',\n        'article',\n        '.main-content',\n        '.article-content',\n        '.post-content',\n        '.entry-content',\n        '#content',\n        '.content'\n    ]\n    \n    for selector in content_selectors:\n        content_area = soup.select_one(selector)\n        if content_area and len(content_area.find_all('p')) > 3:\n            print(f\"\ud83d\udccd Found content using selector: {selector}\")\n            break\n    \n    # Strategy 2: If no content area found, look for area with most paragraphs\n    if not content_area or len(content_area.find_all('p')) < 3:\n        print(\"\ud83d\udccd Using body and filtering paragraphs by length\")\n        content_area = soup.find('body')\n    \n    # Extract paragraphs and headings\n    paragraphs = []\n    seen_texts = set()  # Avoid duplicates\n    \n    for element in content_area.find_all(['p', 'h2', 'h3', 'li']):\n        text = element.get_text(strip=True)\n        \n        # Filter conditions\n        if (len(text) < 15 or  # Too short\n            text in seen_texts or  # Duplicate\n            text.lower().startswith(('cookie', 'privacy', 'terms', '\u5229\u7528\u898f\u7d04', '\u30d7\u30e9\u30a4\u30d0\u30b7\u30fc')) or  # Legal text\n            'href' in text.lower() or  # Likely a link\n            text.count('|') > 2):  # Navigation menu\n            continue\n        \n        seen_texts.add(text)\n        paragraphs.append(text)\n    \n    print(f\"\u2705 Extracted {len(paragraphs)} unique paragraphs\")\n    \n    # Show first few paragraphs for debugging\n    if paragraphs:\n        print(f\"\\n\ud83d\udccb First 3 paragraphs:\")\n        for i, p in enumerate(paragraphs[:3], 1):\n            preview = p[:80] + \"...\" if len(p) > 80 else p\n            print(f\"   {i}. {preview}\")\n    \n    # Combine text (limit to first 10 paragraphs to stay within token limits)\n    # IMPORTANT: Reduced from 20 to 10 paragraphs for better translation quality\n    full_text = f\"{title_text}\\n\\n\" + \"\\n\\n\".join(paragraphs[:10])\n    \n    return {\n        'title': title_text,\n        'text': full_text,\n        'paragraph_count': len(paragraphs),\n        'paragraphs_used': min(10, len(paragraphs))\n    }\n\n# Extract article\nprint(\"\ud83d\udcc4 Extracting article content...\\n\")\narticle = extract_article_text(ARTICLE_URL)\n\nprint(f\"\\n\u2705 Article summary:\")\nprint(f\"   Title: {article['title']}\")\nprint(f\"   Total paragraphs: {article['paragraph_count']}\")\nprint(f\"   Using paragraphs: {article['paragraphs_used']}\")\nprint(f\"   Text length: {len(article['text'])} characters\\n\")\n\n# Check if we have enough content\nif article['paragraph_count'] < 3:\n    print(\"\u26a0\ufe0f  Warning: Very few paragraphs extracted. The article might not be accessible or requires different extraction logic.\")\n    print(\"   Proceeding with available content...\\n\")\n\n# Load translation backend\nfrom transformers_backend import TransformersBackend\n\nprint(\"\ud83d\udd04 Loading translation backend...\")\nbackend = TransformersBackend()\nbackend.load_model()\n\n# Translate article\nprint(f\"\\n\ud83d\udd04 Translating article...\")\nprint(f\"Source language: {SOURCE_LANG} \u2192 Target language: {TARGET_LANG}\\n\")\n\nstart_time = time.time()\n\n# Monkey patch to increase max_new_tokens and add debug output\nimport torch\noriginal_translate = backend.translate\n\ndef translate_with_more_tokens(text, source_lang, target_lang):\n    \"\"\"Modified translate with more tokens and debug output\"\"\"\n    # Build structured message\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"text\",\n            \"text\": text,\n            \"source_lang_code\": source_lang,\n            \"target_lang_code\": target_lang\n        }]\n    }]\n\n    # Apply chat template\n    inputs = backend.tokenizer.apply_chat_template(\n        messages,\n        return_tensors=\"pt\"\n    ).to(backend.model.device)\n\n    start = time.time()\n\n    # Generate with MORE tokens\n    with torch.no_grad():\n        outputs = backend.model.generate(\n            inputs,\n            max_new_tokens=1024,  # Increased from 256 to 1024\n            do_sample=False\n        )\n\n    duration = time.time() - start\n\n    # Decode full output\n    full_output = backend.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Debug: Show full output\n    print(\"\\n\ud83d\udd0d Debug - Full model output (first 500 chars):\")\n    print(full_output[:500])\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\n    # Extract translation (improved logic)\n    # Strategy 1: Split by newline and get last non-empty line\n    lines = [line.strip() for line in full_output.split('\\n') if line.strip()]\n    translation = lines[-1] if lines else full_output\n    \n    # Strategy 2: Remove prompt prefix if present\n    if ':' in translation and len(translation.split(':', 1)[1].strip()) > 10:\n        translation = translation.split(':', 1)[1].strip()\n\n    # Post-processing: Convert Simplified to Traditional Chinese\n    if target_lang == \"zh-TW\":\n        try:\n            from hanziconv import HanziConv\n            translation = HanziConv.toTraditional(translation)\n        except ImportError:\n            pass\n\n    # Calculate tokens\n    input_tokens = inputs.shape[1]\n    output_tokens = outputs.shape[1] - input_tokens\n    total_tokens = outputs.shape[1]\n\n    return {\n        \"translation\": translation,\n        \"time\": duration,\n        \"tokens\": total_tokens,\n        \"metadata\": {\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens,\n            \"tokens_per_second\": total_tokens / duration if duration > 0 else 0,\n            \"full_output_preview\": full_output[:200]\n        }\n    }\n\nbackend.translate = translate_with_more_tokens\n\nresult = backend.translate(article['text'], source_lang=SOURCE_LANG, target_lang=TARGET_LANG)\nend_time = time.time()\n\n# Display result with word wrap\nimport textwrap\n\nprint(f\"\\n\u2705 Translation Result:\")\nprint(\"=\" * 80)\nwrapped_translation = textwrap.fill(result['translation'], width=80, break_long_words=False, break_on_hyphens=False)\nprint(wrapped_translation)\nprint(\"=\" * 80)\n\nprint(f\"\\n\u23f1\ufe0f  Time: {result['time']:.2f}s\")\nprint(f\"\ud83d\udcca Speed: {result['metadata']['tokens_per_second']:.1f} tok/s\")\nprint(f\"\ud83d\udd24 Tokens: {result['tokens']} (input: {result['metadata']['input_tokens']}, output: {result['metadata']['output_tokens']})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7\ufe0f\u20e3 Website Screenshot Translation\n\nCapture a screenshot of any website and translate it to Traditional Chinese."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install system dependencies for Chromium\n!apt-get update -qq\n!apt-get install -y -qq libatk1.0-0 libatk-bridge2.0-0 libcups2 libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libpango-1.0-0 libcairo2 libasound2\n\n# Install Playwright\n!pip install playwright -q\n!playwright install chromium --with-deps\n\n# Screenshot website and translate\nimport asyncio\nfrom playwright.async_api import async_playwright\nfrom PIL import Image\nimport sys\nsys.path.insert(0, 'examples')\nsys.path.insert(0, 'examples/backends')\n\n# Configuration\nWEBSITE_URL = \"https://www.yomiuri.co.jp/national/20260117-GYT1T00119/\"\nSOURCE_LANG = \"ja\"  # Japanese news website\n\nasync def capture_screenshot(url):\n    \"\"\"Capture website screenshot using Playwright async API\"\"\"\n    print(f\"\ud83d\udcf8 Capturing screenshot of: {url}\\n\")\n    \n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page(viewport={'width': 1280, 'height': 1024})\n        await page.goto(url, wait_until='networkidle')\n        await page.screenshot(path='website_screenshot.png', full_page=False)\n        await browser.close()\n    \n    print(\"\u2705 Screenshot saved: website_screenshot.png\\n\")\n\n# Capture screenshot\nawait capture_screenshot(WEBSITE_URL)\n\n# Load backend\nfrom transformers_multimodal_backend import TransformersMultimodalBackend\n\nprint(\"\ud83d\udd04 Loading multimodal backend...\")\nbackend = TransformersMultimodalBackend()\nbackend.load_model()\n\n# Translate\nprint(f\"\\n\ud83d\udd04 Translating screenshot...\")\nprint(f\"Source language: {SOURCE_LANG} \u2192 Target language: {TARGET_LANG}\\n\")\nresult = backend.translate_image('website_screenshot.png', source_lang=SOURCE_LANG, target_lang=TARGET_LANG)\n\n# Display result\nprint(f\"\\n\u2705 Translation:\")\nprint(result['translation'])\nprint(f\"\\n\u23f1\ufe0f  Time: {result['time']:.2f}s, Speed: {result['metadata']['tokens_per_second']:.1f} tok/s\")\n\n# Display screenshot\nfrom IPython.display import Image as IPImage, display\ndisplay(IPImage('website_screenshot.png', width=800))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Notes\n",
    "\n",
    "- **Backend**: `transformers` is best for Colab GPU (T4)\n",
    "- **Target Language**: Default is `zh-TW` (Traditional Chinese), change in Configuration section\n",
    "- **Image Mode**: Slower but preserves visual context (tables, charts, layout)\n",
    "- **DPI**: Lower DPI (72-96) is faster, higher DPI (150) has better quality\n",
    "\n",
    "## \ud83d\udd17 Links\n",
    "\n",
    "- [GitHub Repository](https://github.com/jimmyliao/trans-gemma)\n",
    "- [TranslateGemma Model](https://huggingface.co/google/translategemma-4b-it)\n",
    "- [Documentation](https://github.com/jimmyliao/trans-gemma/blob/main/examples/README.md)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}